{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfejMHs4lr8V"
   },
   "source": [
    "*Copyright 2024 The Penzai Authors.*\n",
    "\n",
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at*\n",
    "\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "*Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USGIPdLYDzSo"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/penzai/blob/main/notebooks/gemma_from_scratch.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google-deepmind/penzai/blob/main/notebooks/gemma_from_scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhrodVOyBdi7"
   },
   "source": [
    "# Gemma From Scratch: Inspecting and Re-implementing Gemma with Penzai\n",
    "\n",
    "Penzai includes a number of general-purpose tools for analyzing JAX neural networks. It also includes a declarative neural-network library designed to take advantage of those tools. This notebook demonstrates how to apply this tooling to a real-world neural network: the Gemma pretrained transformer model, [implemented in Flax](https://github.com/google-deepmind/gemma).\n",
    "\n",
    "You might benefit from reading this notebook if any of these apply:\n",
    "\n",
    "- You are interested in learning about Penzai's design principles, especially if you are already familiar with Flax.\n",
    "- You want to reverse-engineer a model that is currently written in Flax using Penzai's tools.\n",
    "- You want to implement a model in Penzai, and would like to learn about the best practices for model development.\n",
    "- You want to learn more about the Gemma implementation included in Penzai, which is used by the other tutorial notebooks.\n",
    "\n",
    "This notebook is broken into three main sections plus a setup section.\n",
    "\n",
    "- In **Section 0**, we set up the environment and load the Gemma weights for further analysis.\n",
    "- In **Section 1**, we show how to apply Penzai's analysis and visualization tooling to the official Flax implementation of Gemma, and how to convert it into a Penzai-compatible form. We also discuss the high-level differences between Flax and Penzai.\n",
    "- In **Section 2**, we break down Gemma's training/scoring mode into its constituent pieces, and show how to re-implement each of those pieces in idiomatic Penzai style. We also discuss how Penzai enables easy inspection of intermediate values, and use intermediate values to test the implementation of each piece.\n",
    "- In **Section 3**, we apply the same decomposition to the stateful key-value-caching mode, and demonstrate how stateful operations are represented in Penzai models using Penzai's \"data effects\" system. We use this to implement a simple JIT-compiled sampler that still supports patching intermediate activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toWyv7wiIUhY"
   },
   "source": [
    "## Section 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mauxbcy-O0wi"
   },
   "source": [
    "In this section, we'll start by setting up our environment and loading the Gemma model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGdN48kHPXTg"
   },
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmxgAcFQmZkB"
   },
   "source": [
    "To run this notebook, you need a Python environment with `penzai` and its dependencies installed.\n",
    "\n",
    "In Colab or Kaggle, you can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGZH58j8mPkj"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import penzai\n",
    "except ImportError:\n",
    "  !pip install penzai[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cLpSHM_L39Q"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import gemma\n",
    "except ImportError:\n",
    "  !pip install \"gemma @ git+https://www.github.com/google-deepmind/gemma.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iog3oMAMGCMG"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any\n",
    "\n",
    "import os\n",
    "import dataclasses\n",
    "import traceback\n",
    "import functools\n",
    "import gc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import orbax.checkpoint\n",
    "import chex\n",
    "\n",
    "import flax.linen\n",
    "from jax.experimental import mesh_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljCMQGV00mZ1"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQU6QfoZVYOL"
   },
   "outputs": [],
   "source": [
    "import gemma.params\n",
    "import gemma.sampler\n",
    "import gemma.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Mh2mAuiQ4aa"
   },
   "outputs": [],
   "source": [
    "import penzai\n",
    "from penzai import pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7L1h-w3Sx9Y"
   },
   "outputs": [],
   "source": [
    "import penzai.toolshed.unflaxify\n",
    "import penzai.toolshed.isolate_submodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOPMLLCyPbdA"
   },
   "source": [
    "### Loading Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFtr5l_ZVAI7"
   },
   "source": [
    "Next we can load the Gemma model, using its official Flax reference implementation. We'll use Gemma 2B for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0X3qrwbK4SvX"
   },
   "source": [
    "You can download the Gemma checkpoints using a Kaggle account and an API key. If you don't have an API key already, you can:\n",
    "\n",
    "1. Visit https://www.kaggle.com/ and create an account if needed.\n",
    "2. Go to your account settings, then the 'API' section.\n",
    "3. Click 'Create new token' to download your key.\n",
    "\n",
    "Next, if you are running this notebook in Google Colab:\n",
    "\n",
    "1. Click the \"key\" symbol on the left toolbar to open the \"Secrets\" tab.\n",
    "2. Add two new secrets, named \"KAGGLE_USERNAME\" and \"KAGGLE_KEY\", and set their values based on the API key you downloaded.\n",
    "3. Run the cell below and grant this notebook access to the secrets you just made.\n",
    "\n",
    "If you are not running this notebook in Google Colab, you can instead run the cell below, input your username and API key in the textboxes, and click the login button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNgVlaJl2IbZ"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  kagglehub.config.set_kaggle_credentials(\n",
    "      userdata.get(\"KAGGLE_USERNAME\"), userdata.get(\"KAGGLE_KEY\")\n",
    "  )\n",
    "except ImportError:\n",
    "  kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8Oxko4R4yaK"
   },
   "source": [
    "If everything went well, you should see:\n",
    "\n",
    "```\n",
    "Kaggle credentials set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUnej-yy5beB"
   },
   "source": [
    "Before downloading Gemma, you will also need to consent to the Gemma Terms of Use. If you haven't done that yet, you can do so here:\n",
    "\n",
    "> https://www.kaggle.com/models/google/gemma/license/consent\n",
    "\n",
    "(Make sure you choose to \"Verify via Kaggle Account\" with the same account you used to log in above!)\n",
    "\n",
    "Once you've agreed to the terms, you can run the next cell to download the Gemma weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmUAGwXE41la"
   },
   "outputs": [],
   "source": [
    "weights_dir = kagglehub.model_download('google/gemma/Flax/2b')\n",
    "ckpt_path = os.path.join(weights_dir, '2b')\n",
    "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nlbX3C-1sB6"
   },
   "source": [
    "We can then load the SentencePiece vocabulary and restore the checkpointed parameters into JAX using `orbax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWexi3K5Vnkj"
   },
   "outputs": [],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VU6Qjh2UVznN"
   },
   "outputs": [],
   "source": [
    "checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "structure = checkpointer.metadata(ckpt_path)\n",
    "\n",
    "sharding = jax.sharding.SingleDeviceSharding(jax.local_devices()[0])\n",
    "restore_args = jax.tree_util.tree_map(\n",
    "    lambda m: orbax.checkpoint.ArrayRestoreArgs(\n",
    "        restore_type=jax.Array, sharding=sharding\n",
    "    ),\n",
    "    structure,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJg-U3eFYks2"
   },
   "outputs": [],
   "source": [
    "flat_params = checkpointer.restore(ckpt_path, restore_args=restore_args)\n",
    "params = gemma.params.nest_params(\n",
    "    gemma.params.param_remapper(flat_params)\n",
    ")\n",
    "del flat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emqJKewuYm6N"
   },
   "outputs": [],
   "source": [
    "flax_gemma_config = gemma.transformer.TransformerConfig.from_params(\n",
    "    params, cache_size=1024\n",
    ")\n",
    "flax_gemma = gemma.transformer.Transformer(flax_gemma_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dRVcavPPc9Y"
   },
   "source": [
    "## Section 1: Analyzing the Flax model with Penzai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_7jrUZ-bIZg"
   },
   "source": [
    "In this section, we'll give an overview of Penzai's analysis and visualization tooling and demonstrate how to apply it to existing Flax models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk2KwUZ_b6x8"
   },
   "source": [
    "### Looking at the model and its weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i-fKD0Yb-_t"
   },
   "source": [
    "Penzai ships with a powerful pretty-printer and array visualizer designed to help you quickly navigate through and understand the structure of large trees. If you've used Colab or Jupyter before, you may be familiar with printouts that look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PX7ZZlzDa6AF"
   },
   "outputs": [],
   "source": [
    "print(repr(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9ML6i3CdhPV"
   },
   "source": [
    "Penzai provides an interactive alternative, `treescope`, which allows you to interactively fold and unfold children of deep trees like this. Try clicking on the gray triangle markers to expand or contract subtrees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zyD2Nt7dBT8"
   },
   "outputs": [],
   "source": [
    "pz.show(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5UWuP9Td8dy"
   },
   "source": [
    "Let's turn on `penzai.treescope` as the default Colab/IPython:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG_fkKVkBZNb"
   },
   "outputs": [],
   "source": [
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUry66c_eLgH"
   },
   "source": [
    "Now everything we return from a Colab cell will be interactively pretty-printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IpNIG28eQYH"
   },
   "outputs": [],
   "source": [
    "params[\"transformer\"][\"layer_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhiR4liFb9xW"
   },
   "source": [
    "`penzai.treescope` also includes an n-dimensional array visualizer, which can help you understand the shape and content of arrays at a glance. You can hover or click on the cells of the visualization to inspect individual array elements.\n",
    "\n",
    "Note that, with the `truncate=True` argument, we automatically cut out the middle elements of each array to keep the visualization a reasonable size. This is similar to how printing out an array produces `...` in the middle of array printouts for large arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvAGkYLCe_oj"
   },
   "outputs": [],
   "source": [
    "pz.ts.render_array(params[\"transformer\"][\"layer_0\"][\"attn\"][\"q_einsum\"][\"w\"], truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKo9ewbmfdKD"
   },
   "source": [
    "Since we ran `register_autovisualize_magic` above, we can also automatically visualize arrays whenever we return something from a Colab cell using the `%%autovisualize` magic command. Treescope will automatically insert these visualizations inside the rendered tree itself and let you expand them as desired. Try clicking the triangles to look at different weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XECEVJUXfGUN"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "params[\"transformer\"][\"layer_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffmlt_kOhZ95"
   },
   "source": [
    "We can also use treescope to print out the model itself. However, Flax models don't show much about themselves when you construct them. In this case, the transformer model is a Python dataclass whose attributes are just it's configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIQf93B6fwP8"
   },
   "outputs": [],
   "source": [
    "flax_gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO5RIwekhxwP"
   },
   "source": [
    "(You get something similar if you print it out without Treescope:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgfZ9-HLhsQ0"
   },
   "outputs": [],
   "source": [
    "print(flax_gemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-MU9XVoiHwi"
   },
   "source": [
    "Unfortunately, you can't directly see the individual layers inside the Gemma model here, because in Flax those layers aren't actually built until you call `apply` on the model and bind it to parameters. But as we will see later, we can use Penzai to get a better look at the internals of the Gemma model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8l75MPEBh7NS"
   },
   "source": [
    "### Looking at model inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zVAcrCNiCb7"
   },
   "source": [
    "Let's run the model on some example text. We'll start by tokenizing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiuYM9Odh1Kt"
   },
   "outputs": [],
   "source": [
    "example_input = \"Penzai includes a number of general-purpose tools for analyzing JAX neural networks. It also includes a declarative neural-network library designed to take advantage of those tools. This notebook demonstrates how to apply this tooling to a real-world neural network: the Gemma pretrained transformer model.\"\n",
    "print(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhJ5N6YjjuVe"
   },
   "outputs": [],
   "source": [
    "tokens = jnp.array([vocab.bos_id()] + vocab.EncodeAsIds(example_input))\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biXwYUQlkiw2"
   },
   "source": [
    "We can apply treescope's array visualizer to tokens too! Discrete data is shown using colors, with stripes used for numbers with a lot of digits. (Fun fact: since sentencepiece tokenizers tend to give lower IDs to more common tokens, more common tokens tend to have simpler-looking visualizations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I23Rwr_8kdSX"
   },
   "outputs": [],
   "source": [
    "pz.ts.render_array(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaFIjS63k-jX"
   },
   "source": [
    "In fact, we can even pass the tokenizer to the autovisualizer, in which case hovering or clicking on array elements will tell you what token each ID is for. Try it below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4Kw07T0kqmW"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VIlk0VmGGQ"
   },
   "source": [
    "Now let's call the model on it (adapting the logic from [this notebook](https://colab.sandbox.google.com/github/google-deepmind/gemma/blob/main/colabs/fine_tuning_tutorial.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKisHsrKlqeF"
   },
   "outputs": [],
   "source": [
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "  pad_mask = example != pad_id\n",
    "  current_token_position = gemma.transformer.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = gemma.transformer.make_causal_attn_mask(pad_mask)\n",
    "  return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXp5VJIEmU3R"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "positions, attention_mask = get_attention_mask_and_positions(tokens[None, :], vocab.pad_id())\n",
    "\n",
    "flax_gemma_output, new_vars = flax_gemma.apply(\n",
    "    {'params': params['transformer']},\n",
    "    tokens[None, :],\n",
    "    positions,\n",
    "    None, # Attention cache is None.\n",
    "    attention_mask,\n",
    ")\n",
    "assert new_vars is None\n",
    "flax_gemma_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwwIugrFmtjf"
   },
   "source": [
    "This visualization shows up in mostly red, because most of Gemma's output logits are negative. We can map this to a probability distribution using softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnUiWEocmchh"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "jax.nn.softmax(flax_gemma_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnvV66o0m8Xg"
   },
   "source": [
    "Let's find the most-likely prediction at each position, and compare it to the actual tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euFss0xnm0nf"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "predictions = jnp.argmax(flax_gemma_output, axis=-1)\n",
    "jnp.stack([predictions[0, :-1], tokens[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRTUzIvonCSv"
   },
   "outputs": [],
   "source": [
    "list(zip([vocab.IdToPiece(tok) for tok in predictions[0].tolist()], [vocab.IdToPiece(tok) for tok in tokens[1:].tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2prTo3tioTmo"
   },
   "source": [
    "### Looking inside the Flax model with Flax utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r52lJKKlo5Tt"
   },
   "source": [
    "How can we figure out what is happening inside Gemma? Of course, we can look at the code to see how it's implemented, but what if we want to see intermediate activations, or inspect how data flows between Flax modules?\n",
    "\n",
    "Flax does include a few utilities for this, which are described in the [Flax guides](https://flax.readthedocs.io/en/latest/guides/index.html) and don't require using Penzai. One option is to use \"tabulate\" to list out all of the submodule calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ReILixQAnSBo"
   },
   "outputs": [],
   "source": [
    "print(flax_gemma.tabulate(\n",
    "    jax.random.key(42),\n",
    "    tokens[None, :],\n",
    "    positions,\n",
    "    None, # Attention cache is None.\n",
    "    attention_mask,\n",
    "    console_kwargs={\"width\": 120}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BlmgQQGqpje"
   },
   "source": [
    "Another option is to use `capture_intermediates` to return intermediate activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp8RD6h7pz7Y"
   },
   "outputs": [],
   "source": [
    "flax_gemma.apply(\n",
    "    {'params': params['transformer']},\n",
    "    tokens[None, :],\n",
    "    positions,\n",
    "    None, # Attention cache is None.\n",
    "    attention_mask,\n",
    "    capture_intermediates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrO4Vm3arTNo"
   },
   "source": [
    "Flax also includes an advanced [\"intercept_methods\" utility](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.intercept_methods) which allows you to intercept module calls and apply custom logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOGtOmpnru9_"
   },
   "source": [
    "### Flax models v.s. Penzai models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGP2jGdprw85"
   },
   "source": [
    "What if we want to do more complex operations, like looking at the inputs passed to the submodules, changing the output of submodules, or extracting and running submodules individually? This is possible using `flax.linen.intercept_methods`, but it can be somewhat difficult to reason about. An alternative is to convert the Flax model to a Penzai model, and then use Penzai's tree-rewriting tools to visualize things. To this end, Penzai provides a utility `unflaxify` which recursively intercepts every Flax method call and encapsulates it into an equivalent Penzai layer.\n",
    "\n",
    "Before we show how this works, let's briefly pause to discuss the differences between Penzai models and Flax models, and the overall differences between the Flax and Penzai conventions and design ideas.\n",
    "\n",
    "From the [\"Flax philosophy\" documentation](https://flax.readthedocs.io/en/latest/philosophy.html), Flax aims to \"*offer an API familiar to those experienced with Keras/Sonnet/PyTorch*\" with \"*an implicit variable management API to save the user from having to manually thread thousands of variables through a complex tree of functions.*\" To this end, Flax modules are defined as if they own stateful variables and parameters, which they can modify imperatively, and Flax runs logic under-the-hood to transform this stateful view into a functional computation that works with JAX. Flax modules generally look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPpIjsx4rFmZ"
   },
   "outputs": [],
   "source": [
    "Initializer = jax.nn.initializers.Initializer\n",
    "\n",
    "class SimpleFlaxDense(flax.linen.Module):\n",
    "  features: int\n",
    "  kernel_init: Initializer = flax.linen.initializers.lecun_normal()\n",
    "  bias_init: Initializer = flax.linen.initializers.zeros_init()\n",
    "\n",
    "  @flax.linen.compact\n",
    "  def __call__(self, inputs):\n",
    "    kernel = self.param('kernel',\n",
    "                        self.kernel_init, # Initialization function\n",
    "                        (inputs.shape[-1], self.features))  # Shape info.\n",
    "    y = jnp.dot(inputs, kernel)\n",
    "    bias = self.param('bias', self.bias_init, (self.features,))\n",
    "    y = y + bias\n",
    "    return y\n",
    "\n",
    "class SimpleFlaxMLP(flax.linen.Module):\n",
    "  out_dims: int\n",
    "\n",
    "  @flax.linen.compact\n",
    "  def __call__(self, x):\n",
    "    x = SimpleFlaxDense(128)(x)\n",
    "    x = flax.linen.relu(x)\n",
    "    x = SimpleFlaxDense(self.out_dims)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwjSLjFR3qIN"
   },
   "outputs": [],
   "source": [
    "SimpleFlaxMLP(out_dims=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWZCVijX3qzf"
   },
   "outputs": [],
   "source": [
    "flax_mlp_params = SimpleFlaxMLP(out_dims=32).init(jax.random.key(10), jnp.ones((4, 8)))\n",
    "flax_mlp_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ujN9D8DMb1C"
   },
   "outputs": [],
   "source": [
    "SimpleFlaxMLP(out_dims=32).apply(flax_mlp_params, jnp.ones((4, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGxxn3-VvdOr"
   },
   "source": [
    "This approach makes Flax a great choice for quickly writing neural networks in JAX, expecially if you are already familiar with stateful neural network libraries and object-oriented programming.\n",
    "\n",
    "However, since this representation was designed for *writing* model architectures, it's not necessarily the best choice for *analyzing* or *patching* those models. Indeed, any such analysis must be designed to work around the transformation from stateful object-oriented modules to functional JAX-compatible method calls.\n",
    "\n",
    "Penzai, on the other hand, prioritizes **analysis**, **visualization**, and **patchability**. One of the primary design goals for Penzai's neural net library `penzai.nn` is to be a *declarative* system where \"*what you see is what you get*\": you should be able to immediately see what your model is going to do when you call it, and you should be able to \"reach in\" and change what it does. This leads to a number of concrete differences:\n",
    "\n",
    "- Parameters:\n",
    "  - In Flax, you define parameters by calling `self.param`, and you can define other mutable variables using `self.variable`. This variable is implicitly inserted into a parameter dictionary and retrieved when a module is functionalized.\n",
    "  - In Penzai, parameters are simply stored as attributes on the layer that owns them. You can walk the tree of layers to extract the parameters and put them in a dictionary if you want, and Penzai makes it easy to do this, but Penzai itself doesn't require it. This also means you can just look at the parameters by looking at your model object.\n",
    "    - If you're familiar with [Equinox](https://github.com/patrick-kidger/equinox), Penzai layers are PyTrees in the same way that Equinox models are, so you can just pass them through JAX transformations without issues. But one difference from Equinox is that all parameters are explicitly marked with a `Parameter` class, and the best practice is to filter for `Parameter` instances instead of implicitly assuming all float-dtype arrays  are parameters.\n",
    "- Submodules:\n",
    "  - In Flax, when you instantiate a new module inside another module's `setup` or `@compact` method, that module is implicitly attached to the containing module, and given its own sub-dictionary of parameters. Each module instance remembers which parameters belong to it in an internal `scope` attribute and looks them up as needed.\n",
    "  - In Penzai, sublayers are simply stored as attributes on the layer that owns them. Each layer already owns its own parameters, so there's no need to do a functionalization step.\n",
    "- Module construction and submodule configuration:\n",
    "  - Flax explicitly supports a `compact` configuration style, where submodules are implicitly configured at the same place where they are called, directly in Python code. This makes it easy to write the model, but somewhat difficult if you want to change a small part of an existing model, since the submodules may not even exist until they are called.\n",
    "  - In Penzai, there is a strict separation between configuring a model and calling it, like Equinox and PyTorch. In fact, Penzai goes even further than other frameworks and tries to be *as permissive as possible* about sublayers after models are configured. Most control flow in Penzai models is explicitly represented in the model's structure using general-purpose combinators like `pz.nn.Sequential` or `pz.nn.Residual`, rather than being implemented as code. This makes it slightly more verbose to write and configure the model, but makes it much easier to visualize and patch it afterward.\n",
    "- Mutable state and random numbers:\n",
    "  - Flax tries to provide a familiar object-oriented interface to stateful operations when the model runs, including mutable variables as part of the core abstraction. Every module has built-in support for variables and states, with a fixed API. Because of this, every module has to know about the variables and states of their submodules, and every top-level module has to be transformed using `.apply` in order to be called in a functional way.\n",
    "  - Penzai intentionally avoids baking mutable variables or random numbers into the core system, and in fact doesn't have any equivalent of `apply`; Penzai models are purely-functional by default. You can directly call methods on your model without doing any wrapping. (This is again a shared feature with Equinox.)\n",
    "    - Penzai does allow you to *opt in* to mutable state or stateful random number generation, however, using a \"data-effects\" system heavily inspired by effect systems in functional programming languages. This system works by rewriting your model tree directly: effects are stored as attributes of your model, and effect handlers inject new values for those attributes using functional type-dependent tree traversals. This means you can freely modify how effects are interpreted, e.g. by \"freezing\" mutable states to specific values, or intercepting state updates.\n",
    "    - Penzai tries very hard to avoid having any hidden or implicit state when your model runs. All mutable states and other side effects are accessed through your model's explicitly-declared attributes, and you can always see them by printing out your model tree in treescope.\n",
    "    - Penzai also tries very hard to avoid changing Python semantics, and doesn't secretly override class construction, modify dataclass attributes, or wrap your instance methods (although in some cases it asks you to explicitly wrap them yourself). You shouldn't have to learn a new dialect of Python to understand Penzai code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqG16-2h3Zok"
   },
   "source": [
    "A direct Penzai equivalent of the Flax dense layer and MLP defined above might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYW3S6v6va4V"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class SimplePzDense(pz.Layer):\n",
    "  kernel: pz.nn.ParameterLike[jax.Array]\n",
    "  bias: pz.nn.ParameterLike[jax.Array]\n",
    "\n",
    "  @pz.checked_layer_call\n",
    "  def __call__(self, inputs):\n",
    "    \"\"\"Calls the dense layer.\"\"\"\n",
    "    y = jnp.dot(inputs, self.kernel.value)\n",
    "    y = y + self.bias.value\n",
    "    return y\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(\n",
    "      cls,\n",
    "      in_features: int,  # <- Requires passing explicit input feature dimension\n",
    "      out_features: int,\n",
    "      kernel_init: Initializer = flax.linen.initializers.lecun_normal(),\n",
    "      bias_init: Initializer = flax.linen.initializers.zeros_init(),\n",
    "  ) -> SimplePzDense:\n",
    "    \"\"\"Builds the dense layer from configuration.\"\"\"\n",
    "    kernel = pz.nn.UninitializedParameter(\n",
    "        initializer=lambda key: kernel_init(key, (in_features, out_features,)),\n",
    "        name=\"kernel\"\n",
    "    )\n",
    "    bias = pz.nn.UninitializedParameter(\n",
    "        initializer=lambda key: bias_init(key, (out_features,)),\n",
    "        name=\"bias\"\n",
    "    )\n",
    "    return cls(kernel=kernel, bias=bias)\n",
    "\n",
    "  # Optional shape-checking methods:\n",
    "  def input_structure(self):\n",
    "    in_features, _ = self.kernel.value_structure.shape\n",
    "    return pz.chk.ArraySpec(\n",
    "        (*pz.chk.var(\"batch\"), in_features), dtype=jnp.floating)\n",
    "  def output_structure(self):\n",
    "    _, out_features = self.kernel.value_structure.shape\n",
    "    return pz.chk.ArraySpec(\n",
    "        (*pz.chk.var(\"batch\"), out_features), dtype=jnp.floating)\n",
    "\n",
    "\n",
    "@pz.pytree_dataclass\n",
    "class SimplePzMLP(pz.nn.Sequential):\n",
    "  sublayers: list[pz.LayerLike]\n",
    "\n",
    "  # __call__ is inherited from pz.nn.Sequential and just runs the children in\n",
    "  # sequence.\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, in_dims: int, out_dims: int) -> SimplePzMLP:\n",
    "    # Penzai doesn't automatically make parameter names unique; you are in\n",
    "    # charge of naming your parameters.\n",
    "    return cls(sublayers=[\n",
    "        pz.nn.add_parameter_prefix(\n",
    "            \"SimplePzDense_0\", SimplePzDense.from_config(in_dims, 128)\n",
    "        ),\n",
    "        pz.nn.Elementwise(jax.nn.relu),\n",
    "        pz.nn.add_parameter_prefix(\n",
    "            \"SimplePzDense_1\", SimplePzDense.from_config(128, out_dims)\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBsBrINBHExk"
   },
   "outputs": [],
   "source": [
    "pz_mlp_def = SimplePzMLP.from_config(in_dims=8, out_dims=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOMMBVeM4-3B"
   },
   "source": [
    "Printing out this model shows you its structure, even before we initialize the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDs00gTX5D8E"
   },
   "outputs": [],
   "source": [
    "pz_mlp_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWdRIXZhHL87"
   },
   "source": [
    "We can initialize parameters by finding all of the UninitializedParameters and calling their initializers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXo7alK7HTk-"
   },
   "outputs": [],
   "source": [
    "with pz.RandomStream(jax.random.key(42)) as stream:\n",
    "  pz_mlp = (\n",
    "      pz.select(pz_mlp_def)\n",
    "      .at_instances_of(pz.nn.UninitializedParameter)\n",
    "      .apply(lambda param: param.initialize(stream.next_key()))\n",
    "  )\n",
    "pz_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImGyNyySMUp5"
   },
   "outputs": [],
   "source": [
    "pz_mlp(jnp.ones((4, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1aDpQqvIPOm"
   },
   "source": [
    "But Penzai provides utilities to do most common tasks for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gYZi_5XImtS"
   },
   "outputs": [],
   "source": [
    "pz_mlp = pz.nn.initialize_parameters(pz_mlp_def, jax.random.key(42))\n",
    "pz_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ctzjgk45EgM"
   },
   "source": [
    "And Penzai provides powerful tools for traversing this structure to do arbitrary transformations. For instance, you can pull out a parameter dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQJ0Q1Ha5JYL"
   },
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    param.name: param\n",
    "    for param in pz.select(pz_mlp).at_instances_of(pz.nn.Parameter).get_sequence()\n",
    "}\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqP8_0HUI8bH"
   },
   "source": [
    "Or substitute initialized parameters back into an uninitialized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vzTocKJWI-fu"
   },
   "outputs": [],
   "source": [
    "pz.select(pz_mlp_def).at_instances_of(pz.nn.UninitializedParameter).apply(lambda param: param_dict[param.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuNiyP9_5JzV"
   },
   "source": [
    "Or even insert new logic to do arbitrary things while the model runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSwM7fqZ5KgZ"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class HelloWorld(pz.Layer):\n",
    "  def __call__(self, inputs):\n",
    "    pz.show(\"Hello world! My intermediate value is:\", inputs)\n",
    "    return inputs\n",
    "\n",
    "hello_world_mlp = (\n",
    "    pz.select(pz_mlp).at_instances_of(pz.nn.Elementwise)\n",
    "    .insert_after(HelloWorld())\n",
    ")\n",
    "hello_world_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0j_PYbU8Mk9Z"
   },
   "outputs": [],
   "source": [
    "hello_world_mlp(jnp.ones((4, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyxEl1wtbg8s"
   },
   "source": [
    "Note that none of these operations actually modify the original `pz_mlp_def`! Instead, each of these operations makes a *copy* with those modifications applied. This means you can safely make complex destructive modifications to your model, because you're only modifying a copy. (If you're familiar with JAX's [`at[...].set(...)` notation](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html), this is basically the same idea extended to full model trees.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqdgwJA05K48"
   },
   "source": [
    "As a sidenote, this isn't quite an idiomatic Penzai model yet. A more idiomatic version would split up `SimpleDense` into separate `Linear` and `AddBias` sublayers, and use Penzai's named axis system so that the meaning of the different axis indices is more obvious. Here's an equivalent but more idiomatic example MLP from `penzai.example_models`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zaXGflR6I2P"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "import penzai.example_models.simple_mlp\n",
    "idiomatic_mlp = pz.nn.initialize_parameters(\n",
    "    penzai.example_models.simple_mlp.MLP.from_config(feature_sizes=[8, 128, 32]),\n",
    "    jax.random.key(42),\n",
    ")\n",
    "idiomatic_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eZYbON5NjDR"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "idiomatic_mlp(pz.nx.wrap(jnp.ones((4, 8)), \"my_batch_dim\", \"features\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQDZVYaJPEQs"
   },
   "source": [
    "Most of the layers in this MLP are subclasses of `pz.nn.Sequential`. We can easily flatten them without changing the behavior of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgeAGI1RPMIT"
   },
   "outputs": [],
   "source": [
    "flat_mlp = pz.nn.inline_groups(\n",
    "    pz.nn.Sequential([idiomatic_mlp]),\n",
    "    parent_filter=lambda _: True, child_filter=lambda _: True)\n",
    "flat_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLHwYJd86Jde"
   },
   "source": [
    "### Looking inside Flax models with Penzai tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDQskmH86fmg"
   },
   "source": [
    "Luckily, we don't need to manually rewrite every Flax model to take advantage of Penzai's tooling. Penzai ships with a utility `penzai.toolshed.unflaxify`, which uses Flax's `intercept_methods` hook to transform a Flax model into an equivalent Penzai one. This converted model isn't usually very idiomatic, but since it fits with Penzai's conventions, you can use Penzai's tools to analyze and visualize it.\n",
    "\n",
    "Let's start by trying it with the Flax MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjqJMRmVSKOH"
   },
   "outputs": [],
   "source": [
    "from penzai.toolshed import unflaxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrrmP6MP6fGZ"
   },
   "outputs": [],
   "source": [
    "intercepted_flax_mlp = unflaxify.unflaxify_apply(\n",
    "    SimpleFlaxMLP(out_dims=32),\n",
    "    flax_mlp_params,\n",
    "    jnp.ones((4, 8))\n",
    ")\n",
    "intercepted_flax_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyiKX9y-Q-b8"
   },
   "source": [
    "`unflaxify` intercepts every module method call, and transforms it into a dataclass object that holds its own parameters and manages its own state. For instance, the two `SimpleFlaxDense` method blocks each are responsible for their own parameters.\n",
    "\n",
    "Calling the intercepted methods still works as you'd expect, although we need to wrap the input argument, because by convention Penzai layers always take a single tree as input instead of taking multiple arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNyYWncFQZ1K"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "intercepted_flax_mlp(unflaxify.ArgsAndKwargs.capture(jnp.ones((4, 8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22q5J_gdSQf1"
   },
   "source": [
    "But we are free to manipulate the tree structure to change this behavior. For instance, let's insert layers that print out intermediates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRjdTdiGSH1l"
   },
   "outputs": [],
   "source": [
    "def add_intermediate_loggers(layer):\n",
    "  \"\"\"Recursively add loggers to this layer and all its sublayers.\"\"\"\n",
    "  return pz.nn.Sequential([\n",
    "      HelloWorld(),\n",
    "      (\n",
    "          pz.select(layer).at_children().at_instances_of(pz.Layer)\n",
    "          .apply(add_intermediate_loggers)\n",
    "      ),\n",
    "      HelloWorld(),\n",
    "  ])\n",
    "\n",
    "logging_flax_mlp = (\n",
    "    pz.select(intercepted_flax_mlp).at_instances_of(pz.Layer)\n",
    "    .apply(add_intermediate_loggers)\n",
    ")\n",
    "pz.select(logging_flax_mlp).at_instances_of(HelloWorld).show_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbN_zsaxTZ5G"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "logging_flax_mlp(unflaxify.ArgsAndKwargs.capture(jnp.ones((4, 8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBw02ISEUGKR"
   },
   "source": [
    "In fact, Penzai has a utility to quickly get an overview of the intermediate values in a computation by directly interleaving them into the model's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkiIiA78UOLk"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "from penzai.toolshed import interleave_intermediates\n",
    "interleaved = interleave_intermediates.run_and_interleave_intermediates(\n",
    "    intercepted_flax_mlp,\n",
    "    unflaxify.ArgsAndKwargs.capture(jnp.ones((4, 8)))\n",
    ")\n",
    "\n",
    "# Helper to expand the interesting parts of the visualiation:\n",
    "pz.select(interleaved).at_instances_of(\n",
    "    interleave_intermediates.IdentityWithSavedActivations\n",
    ").at(lambda x: x.saved_activations[0]).show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-u-12gMT8LM"
   },
   "source": [
    "We can apply this same tooling to the Gemma implementation to get an overview of the main components. For simplicity, we'll focus on the scoring mode (no KV cache) for now. Try clicking around to explore the model's structure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uBcb1kZTe1U"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "\n",
    "intercepted_gemma = unflaxify.unflaxify_apply(\n",
    "    flax_gemma,\n",
    "    {'params': params['transformer']},\n",
    "    tokens[None, :],\n",
    "    positions,\n",
    "    None,  # Attention cache is None.\n",
    "    attention_mask,\n",
    ")\n",
    "\n",
    "intercepted_gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9pFkrGRbEU3"
   },
   "source": [
    "(You might notice the `WithConstantSideInputs` wrapper, which holds the embedding parameters. This is how Penzai handles parameters that need to be shared between multiple parts of a model. Layers that need access to those shared parameters use `SharedParameterLookup` markers to request access to them, and the `WithConstantSideInputs` substitutes those parameters wherever they are needed when the full model is called. See the [data-effects tutorial](data_effects.ipynb) for more information.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4nHGUn5buQk"
   },
   "source": [
    "Now that we've exposed the submodule method calls in our model's structure, we can use Penzai tooling to inspect parts of the model. For instance, let's capture the intermediate values before and after one of the attention layers. We start by clicking the \"copy\" symbol next to one of the layers we are interested in, which copies the following string:\n",
    "```python\n",
    "(lambda root: root.body.submodule_calls[(3, 'layer_2.__call__')].submodule_calls[(1, 'attn.__call__')])\n",
    "```\n",
    "We can then pass that to `pz.select(...).at(...)` to select that layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvwgByHiU5-q"
   },
   "outputs": [],
   "source": [
    "attn_selection = pz.select(intercepted_gemma).at(\n",
    "    (lambda root: root.body.submodule_calls[(3, 'layer_2.__call__')].submodule_calls[(1, 'attn.__call__')])\n",
    ")\n",
    "attn_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVybj1_Xcayi"
   },
   "source": [
    "Now we use another Penzai utility to pull it out along with its activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_QVooy6EI5j"
   },
   "outputs": [],
   "source": [
    "example_gemma_wrapped_arg = unflaxify.ArgsAndKwargs.capture(\n",
    "    tokens[None, :],\n",
    "    positions,\n",
    "    None,  # Attention cache is None.\n",
    "    attention_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUEGB7xGcQqF"
   },
   "outputs": [],
   "source": [
    "from penzai.toolshed import isolate_submodel\n",
    "captured = isolate_submodel.call_and_extract_submodel(\n",
    "    attn_selection,\n",
    "    example_gemma_wrapped_arg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmDQw6pUce8X"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "captured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht-rmZuMeh2w"
   },
   "source": [
    "This lets us take a peek at the arguments that were passed to this attention layer, and also see the values it returned. We can even reproduce the output in a controlled setting by calling the submodel on the saved activations, which isn't easy to do when using the Flax implementation alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-gNZwuTehqA"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "captured.submodel(captured.saved_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxA5MTwzc6ek"
   },
   "source": [
    "To see the interpretation of these arguments and return values, we can cross-reference this with Gemma's code:\n",
    "\n",
    "```python\n",
    "class Attention(nn.Module):\n",
    "  \"\"\"Attention module.\"\"\"\n",
    "\n",
    "  <...>\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      x: jax.Array,\n",
    "      segment_pos: jax.Array,\n",
    "      cache: LayerCache | None,\n",
    "      attn_mask: jax.Array,\n",
    "  ) -> tuple[LayerCache | None, jax.Array]:\n",
    "    <...>\n",
    "\n",
    "    return new_cache, attn_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTA880hbdees"
   },
   "source": [
    "Can we use this to look at the attention pattern itself? Unfortunately, not directly. Flax only allows you to intercept `linen.Module` method calls, and the attention pattern is computed directly in code, so there's nothing to hook into. (In principle, we could use JAX's Jaxpr tracing machinery to look inside this call, but this isn't yet implemented in Penzai.)\n",
    "\n",
    "In the next section, we'll show how to re-implement Gemma in an idiomatic Penzai style. This will make it possible to look at the attention pattern and make other changes more directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc3lIHNEevgM"
   },
   "source": [
    "## Section 2: Porting the Gemma forward pass to Penzai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vyu898u1e3u9"
   },
   "source": [
    "To get the most out of Penzai's analysis and visualization tools, we'd like to expose as much as possible in the model's tree structure. This would enable us to insert new logic at any stage of the computation, and reduce the need for us to cross-reference the activations we see with the model code.\n",
    "\n",
    "In this section, we'll show how to re-implement each of the building blocks of Gemma as idiomatic Penzai layers, in a way that makes it easier to patch and inspect it afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSL3Ft4XqZ84"
   },
   "source": [
    "### Feed-forward layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTz9vhrTqgp9"
   },
   "source": [
    "We'll start with a relatively straightforward layer: the feedforward layer in each transformer block. Gemma's feedforward layer uses GELU-based gated linear units (GEGLU), as proposed by [Shazeer (2020)](https://arxiv.org/abs/2002.05202). In Flax, [Gemma's feedforward layer](https://github.com/google-deepmind/gemma/blob/f03bf638ae35ecb28a1b31cbe18e3fa628184594/gemma/modules.py#L159C1-L185C19) is defined as:\n",
    "```python\n",
    "class FeedForward(nn.Module):\n",
    "  \"\"\"Feed forward module.\"\"\"\n",
    "  features: int\n",
    "  hidden_dim: int\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    w_gating = self.param(\n",
    "        'gating_einsum',\n",
    "        nn.initializers.zeros_init(),\n",
    "        ((2, self.features, self.hidden_dim)),\n",
    "    )\n",
    "    ff_gate = jnp.dot(x, w_gating[0])\n",
    "    gate_value = nn.gelu(ff_gate)\n",
    "\n",
    "    ff1 = jnp.dot(x, w_gating[1])\n",
    "    activations = gate_value * ff1\n",
    "\n",
    "    w_linear = self.param(\n",
    "        'linear',\n",
    "        nn.initializers.zeros_init(),\n",
    "        (self.hidden_dim, self.features),\n",
    "    )\n",
    "    outputs = jnp.dot(activations, w_linear)\n",
    "\n",
    "    return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwuzwP8GrKtl"
   },
   "source": [
    "The computation in this model is written as a sequence of Python operations. Unfortunately, that makes it hard to extract intermediate computations or patch them. In an idiomatic Penzai model, components like this should usually be broken down into smaller parts to make it easier to manipulate interactively.\n",
    "\n",
    "We'll make the following changes to port this to a Penzai layer:\n",
    "- The three dot products are fairly simple linear operations. We'll rewrite them to use a standard Penzai building block for parameterized linear operations, `pz.nn.Linear`. We'll also explicitly split the 'gating_einsum' parameter into two parameters, instead of having a single parameter and slicing it.\n",
    "- `activations` is computed as the product of two values that were computed independently. We'll factor out this pattern into a general `BranchAndMultiplyTogether` combinator, which runs computations independently and then multiplies them together.\n",
    "- We'll then define a Penzai equivalent of `FeedForward` as a subclass of `pz.nn.Sequential`, a standard Penzai combinator that just runs operations in sequence, and define a new classmethod `from_config` that initializes it. This pattern lets us associate configuration logic with a layer while preserving the ability to go in and insert new logic later.\n",
    "- We'll configure all of these layers to use explicit axis names instead of axis indices. Penzai includes a lightweight \"local\" named axis system, where array dimensions can be referred to by either positional indices or names, and where it's easy to \"transpose\" axes between the positional or named indexing patterns.\n",
    "\n",
    "Here's our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAx5WT5xuN0e"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class BranchAndMultiplyTogether(pz.Layer):\n",
    "  branches: list[pz.LayerLike]\n",
    "\n",
    "  def __call__(self, arg):\n",
    "    if not self.branches:\n",
    "      raise ValueError(\n",
    "          'BranchAndMultiplyTogether requires at least one branch.'\n",
    "      )\n",
    "\n",
    "    running_product = self.branches[0](arg)\n",
    "    for branch in self.branches[1:]:\n",
    "      running_product *= branch(arg)\n",
    "\n",
    "    return running_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XefEtKP9BQH5"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass(has_implicitly_inherited_fields=True)\n",
    "class GemmaFeedForward(pz.nn.Sequential):\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(\n",
    "      cls,\n",
    "      embedding_dim: int,\n",
    "      hidden_dim: int,\n",
    "      dtype: jax.typing.DTypeLike = jnp.float32,\n",
    "  ) -> GemmaFeedForward:\n",
    "    return cls([\n",
    "        BranchAndMultiplyTogether(branches=[\n",
    "            pz.nn.NamedGroup(\"gate\", [\n",
    "              pz.nn.add_parameter_prefix(\"gating_linear\",\n",
    "                  pz.nn.Linear.from_config(\n",
    "                      input_axes={\"embedding\": embedding_dim},\n",
    "                      output_axes={\"neurons\": hidden_dim},\n",
    "                      initializer=pz.nn.zero_initializer,\n",
    "                      dtype=dtype,\n",
    "                  ),\n",
    "              ),\n",
    "              pz.nn.Elementwise(jax.nn.gelu),\n",
    "            ]),\n",
    "            pz.nn.add_parameter_prefix(\"value_linear\",\n",
    "                pz.nn.Linear.from_config(\n",
    "                      input_axes={\"embedding\": embedding_dim},\n",
    "                      output_axes={\"neurons\": hidden_dim},\n",
    "                      initializer=pz.nn.zero_initializer,\n",
    "                      dtype=dtype,\n",
    "                )\n",
    "            ),\n",
    "        ]),\n",
    "        pz.nn.add_parameter_prefix(\"out_linear\",\n",
    "            pz.nn.Linear.from_config(\n",
    "                input_axes={\"neurons\": hidden_dim},\n",
    "                output_axes={\"embedding\": embedding_dim},\n",
    "                initializer=pz.nn.zero_initializer,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OffspM9a70FA"
   },
   "source": [
    "One thing to note about this implementation is that we explicitly add parameter prefixes to each child layer using `pz.nn.add_parameter_prefix`. Penzai does not automatically track variable scoping (since this usually requires some sort of implicit state management), and instead gives you full control of how the parameters in your model are named. Constructor methods like `from_config` should ensure that all parameter names in the submodels they return are locally unique; callers can then use `pz.nn.add_parameter_prefix` to ensure uniqueness when combining submodels.\n",
    "\n",
    "*(Aside: You might wonder, why have parameter names at all, if parameters are stored as attributes in the model? The main answer is that Penzai models are often patched and re-configured after they are constructed, so the specific location of the parameter in the model PyTree may change. Giving all parameters an explicit name means they have a stable identifier that persists even when parts of the model are extracted or replaced, making it easier to e.g. save and restore the parameters from checkpoints.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eFctRJJuOFI"
   },
   "source": [
    "Let's build our layer and see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8qxgTQovSvz"
   },
   "outputs": [],
   "source": [
    "pz_ff_def = GemmaFeedForward.from_config(\n",
    "    embedding_dim=2048,\n",
    "    hidden_dim=16384,\n",
    "    dtype=jnp.bfloat16,\n",
    ")\n",
    "pz_ff_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGSVah8VvS-c"
   },
   "source": [
    "You may notice that the Penzai version's *printed representation* looks a bit similar to the Flax module's *code*. This is no accident! Idiomatic Penzai models follow the \"what you see is what you get\" (WYSIWYG) principle: the runtime behavior of a model should always be directly visible from printing it out in IPython.\n",
    "\n",
    "Now let's capture the intermediates for a feedforward layer in the Flax model, build the equivalent Penzai version, and compare their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxwwjQ8Mv8FD"
   },
   "outputs": [],
   "source": [
    "captured_ff = isolate_submodel.call_and_extract_submodel(\n",
    "    pz.select(intercepted_gemma).at((lambda root: root.body.submodule_calls[(1, 'layer_0.__call__')].submodule_calls[(3, 'mlp.__call__')])),\n",
    "    example_gemma_wrapped_arg\n",
    ")\n",
    "captured_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmzSrH-9Ezhh"
   },
   "outputs": [],
   "source": [
    "captured_ff_params = {\n",
    "    param.name: param.value for param in captured_ff.select().at_instances_of(pz.nn.Parameter).get_sequence()\n",
    "}\n",
    "ff_param_mapping = {\n",
    "    'gating_linear.weights': pz.nx.NamedArray.wrap(\n",
    "            captured_ff_params['layer_0.mlp.gating_einsum'][0]\n",
    "        ).tag(\"embedding\", \"neurons\"),\n",
    "    'value_linear.weights': pz.nx.NamedArray.wrap(\n",
    "            captured_ff_params['layer_0.mlp.gating_einsum'][1]\n",
    "        ).tag(\"embedding\", \"neurons\"),\n",
    "    'out_linear.weights': pz.nx.NamedArray.wrap(\n",
    "            captured_ff_params['layer_0.mlp.linear']\n",
    "        ).tag(\"neurons\", \"embedding\"),\n",
    "}\n",
    "pz_ff = (\n",
    "  pz_ff_def\n",
    "  .select().at_instances_of(pz.nn.UninitializedParameter)\n",
    "  .apply(lambda param: param.initialize_with_value(\n",
    "      ff_param_mapping[param.name], strict_dtype=False,\n",
    "  ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhHxuLa5GX-8"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "named_arg = pz.nx.NamedArray.wrap(\n",
    "    captured_ff.saved_input.args[0]\n",
    ").tag(\"batch\", \"seq\", \"embedding\")\n",
    "pz_ff(named_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTqCq_GgG1-W"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "captured_ff.saved_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERiOg_JoG7FU"
   },
   "outputs": [],
   "source": [
    "chex.assert_trees_all_close(\n",
    "    pz_ff(named_arg).unwrap(\"batch\", \"seq\", \"embedding\"),\n",
    "    captured_ff.saved_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8bBRAugv8Sh"
   },
   "source": [
    "Looks like they match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmsYEfajz1Kc"
   },
   "source": [
    "### Positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsZT4dyyNlKi"
   },
   "source": [
    "Next, lets look at positional embeddings. Gemma uses per-layer rotary positional embeddings (RoPE) as proposed by [Su et al. (2021)](https://arxiv.org/abs/2104.09864). In the [Gemma Flax codebase](https://github.com/google-deepmind/gemma/blob/f03bf638ae35ecb28a1b31cbe18e3fa628184594/gemma/positional_embeddings.py#L45), positional embeddings are just an ordinary Python function, since they don't have any parameters:\n",
    "\n",
    "```python\n",
    "def apply_rope(\n",
    "    inputs: jax.Array,    # [B, L]\n",
    "    positions: jax.Array, # [B, L]\n",
    "    head_dim: int,\n",
    "    max_wavelength: int = _MAX_WAVELENGTH,\n",
    ") -> jax.Array:\n",
    "  \"\"\"Applies RoPE.\"\"\"\n",
    "  fraction = 2 * jnp.arange(0, head_dim // 2) / head_dim\n",
    "  timescale = max_wavelength**fraction\n",
    "\n",
    "  sinusoid_inp = (\n",
    "      positions[..., jnp.newaxis] / timescale[jnp.newaxis, jnp.newaxis, :]\n",
    "  )\n",
    "  sinusoid_inp = sinusoid_inp[..., jnp.newaxis, :]\n",
    "  sin = jnp.sin(sinusoid_inp)\n",
    "  cos = jnp.cos(sinusoid_inp)\n",
    "\n",
    "  first_half, second_half = jnp.split(inputs, 2, axis=-1)\n",
    "  first_part = first_half * cos - second_half * sin\n",
    "  second_part = second_half * cos + first_half * sin\n",
    "  out = jnp.concatenate([first_part, second_part], axis=-1)\n",
    "  return out.astype(inputs.dtype)\n",
    "```\n",
    "You might notice a lot of dimension wrangling! This implementation assumes that `inputs` has shape `(batch, tokens, head_dim)` and `positions` has shape `(batch, tokens)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMXF0md4Svk2"
   },
   "source": [
    "How should we represent this in the Penzai model? There are a few additional considerations to take into account:\n",
    "- The `inputs` and `positions` arguments depend on the training example, but the `head_dim` and `max_wavelength` arguments are configuration arguments. If we want to manipulate the positional embeddings in the same way as other parts of a Penzai model, we'd prefer to separate these.\n",
    "- Penzai models are built out of `pytree_dataclass`es so that JAX knows how to traverse them. In order to put this function into a Penzai combinator like `Sequential`, we'd like to ensure that this is also a `pytree_dataclass`, and that the `head_dim` and `max_wavelength` arguments are treated like part of the model structure, not as dynamic arrays.\n",
    "- The `positions` argument represents the position of each token in the sequence. This argument is constant throughout the entire model, across all of the layers, so we'd prefer not to have to thread it through every layer in the model. Also, by convention, Penzai layers always accept exactly one argument, usually produced by the previous layer.\n",
    "- By convention, Penzai layers do not make assumptions about the number of batch dimensions, and use named axes to refer to only the axes they care about.\n",
    "\n",
    "We can address this by converting `apply_rope` into a non-parameterized Layer, treating `inputs` as the ordinary single-argument input for a Layer, using Penzai's `SideInputEffect` to handle threading through the `positions` argument, and adapting it to use named axes. The resulting implementation is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7cRQZa80gY7"
   },
   "outputs": [],
   "source": [
    "from dataclasses import field\n",
    "\n",
    "@pz.pytree_dataclass\n",
    "class ApplyRoPE(pz.Layer):\n",
    "  # The metadata annotations indicate that these values shouldn't be traversed\n",
    "  # by JAX, and should always have concrete values instead of being traced.\n",
    "  # (The actual PyTree flattening operations are defined by `pz.Struct`, the\n",
    "  # superclass of `pz.Layer`, rather than being baked into pytree_datclass\n",
    "  # itself.)\n",
    "  embedding_axis: str = field(metadata={\"pytree_node\": False})\n",
    "  max_wavelength: int = field(metadata={\"pytree_node\": False})\n",
    "\n",
    "  # The `positions` attribute is a \"side input\". We expect some effect handler\n",
    "  # to replace the value of this attribute with a concrete implementation.\n",
    "  positions: pz.de.SideInputEffect[pz.nx.NamedArray]\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(\n",
    "      cls,\n",
    "      positions_tag: Any,\n",
    "      embedding_axis: str,\n",
    "      max_wavelength: int = 10_000,\n",
    "  ) -> \"ApplyRoPE\":\n",
    "    return cls(\n",
    "        embedding_axis=embedding_axis,\n",
    "        max_wavelength=max_wavelength,\n",
    "        positions=pz.de.SideInputRequest(tag=positions_tag),\n",
    "    )\n",
    "\n",
    "  def _apply_1d(self, input_slice: jax.Array, position: jax.Array) -> jax.Array:\n",
    "    \"\"\"Apply RoPE to a one-dimensional JAX array.\"\"\"\n",
    "    assert input_slice.ndim == 1\n",
    "    assert position.ndim == 0\n",
    "    # Infer `head_dim` from the input shape\n",
    "    [head_dim] = input_slice.shape\n",
    "    fraction = 2 * jnp.arange(0, head_dim // 2) / head_dim\n",
    "    timescale = self.max_wavelength ** fraction\n",
    "    # Since we're assuming `timescale` is a vector and `position` is a scalar,\n",
    "    # we don't need any axis alignment.\n",
    "    sinusoid_inp = position / timescale\n",
    "    sin = jnp.sin(sinusoid_inp)\n",
    "    cos = jnp.cos(sinusoid_inp)\n",
    "    first_half, second_half = jnp.split(input_slice, 2)\n",
    "    first_part = first_half * cos - second_half * sin\n",
    "    second_part = second_half * cos + first_half * sin\n",
    "    return jnp.concatenate([first_part, second_part])\n",
    "\n",
    "  @pz.checked_layer_call\n",
    "  def __call__(self, inputs: pz.nx.NamedArray) -> pz.nx.NamedArray:\n",
    "    # SideInputEffect.ask() is how we retrieve a value from the effect handler.\n",
    "    positions = self.positions.ask()\n",
    "\n",
    "    # Embedding axis should not already be part of the positions.\n",
    "    assert self.embedding_axis not in positions.named_shape\n",
    "    # Every axis of the positions should appear in the inputs.\n",
    "    assert not positions.positional_shape\n",
    "    assert all(axis in inputs.named_shape for axis in positions.named_shape)\n",
    "\n",
    "    # Unbind the embedding axis from the inputs, producing a 1-D\n",
    "    # positional view.\n",
    "    inputs_view = inputs.untag(self.embedding_axis)\n",
    "    # Run the logic over our 1D view using `pz.nmap`, which vectorizes a\n",
    "    # function over all named axes:\n",
    "    out = pz.nx.nmap(self._apply_1d)(inputs_view, positions)\n",
    "    # Finally, re-bind the embedding axis.\n",
    "    out_named = out.tag(self.embedding_axis)\n",
    "    return out_named.astype(inputs.dtype)\n",
    "\n",
    "  # input_structure and output_structure are how we add optional shape and\n",
    "  # structure annotations to our layer. These are checked by the\n",
    "  # checked_layer_call decorator.\n",
    "  def input_structure(self):\n",
    "    return pz.chk.ArraySpec(\n",
    "        named_shape={**pz.chk.var(\"B\"), self.embedding_axis: pz.chk.var(\"F\")},\n",
    "        dtype=np.floating,\n",
    "    )\n",
    "\n",
    "  def output_structure(self):\n",
    "    return self.input_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHmbI-DcaLxy"
   },
   "source": [
    "We can look at it in treescope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew1-E1CUaFG3"
   },
   "outputs": [],
   "source": [
    "ApplyRoPE.from_config(positions_tag=\"positions\", embedding_axis=\"embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb6L9whIWJ5V"
   },
   "source": [
    "Note that the pytree-node fields are shown in italics, whereas the static fields are shown in a normal style. Treescope also helpfully tells us there's an unhandled `SideInputEffect`, which means calling it directly will result in an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aN2iNf7aVn_"
   },
   "outputs": [],
   "source": [
    "layer = ApplyRoPE.from_config(positions_tag=\"positions\", embedding_axis=\"embedding\")\n",
    "try:\n",
    "  layer(pz.nx.ones({\"seq\": 100, \"embedding\": 64}))\n",
    "except pz.de.UnhandledEffectError:\n",
    "  traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgcHP28-cxN9"
   },
   "source": [
    "To provide a value for `positions`, we need to replace the `SideInputRequest` with a concrete value. We can do this using an *effect handler*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdVM7veBatkv"
   },
   "outputs": [],
   "source": [
    "handled_rope_layer = pz.de.WithSideInputsFromInputTuple.handling(\n",
    "    ApplyRoPE.from_config(positions_tag=\"positions\", embedding_axis=\"embedding\"),\n",
    "    tags=[\"positions\"],\n",
    ")\n",
    "handled_rope_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kOkNtb0dVeZ"
   },
   "source": [
    "The class method `pz.de.WithSideInputFromArg.handling` has replaced the `SideInputRequest` with a `HandledSideInputRef`, which indicates that it is responsible for providing the value. We can then call it with a tuple of inputs and positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaqEqsv-dj6S"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "handled_rope_layer((\n",
    "    pz.nx.ones({\"seq\": 50, \"embedding\": 32}),\n",
    "    pz.nx.arange(\"seq\", 50),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iipqmvYhfLLi"
   },
   "source": [
    "Why go through the trouble of using a side input? It makes it easy to pass the same positions argument to multiple layers, without needing to thread it through every intermediate layer. For instance, we can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqoEUjv4fWvc"
   },
   "outputs": [],
   "source": [
    "handled_sequential = pz.de.WithSideInputsFromInputTuple.handling(\n",
    "    pz.nn.Sequential([\n",
    "        ApplyRoPE.from_config(positions_tag=\"positions\", embedding_axis=\"embedding\"),\n",
    "        HelloWorld(),  # <- our \"Hello World\" layer from Section 1\n",
    "        ApplyRoPE.from_config(positions_tag=\"positions\", embedding_axis=\"embedding\"),\n",
    "    ]),\n",
    "    tags=[\"positions\"],\n",
    ")\n",
    "handled_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIPOU_rkgCRz"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "result = handled_sequential((\n",
    "    pz.nx.ones({\"seq\": 50, \"embedding\": 32}),\n",
    "    pz.nx.arange(\"seq\", 50),\n",
    "))\n",
    "pz.show(\"Final result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhaDu0P5foRS"
   },
   "source": [
    "Both of the `AddRoPE` layers received the same positions argument, without either the `Sequential` or `HelloWorld` layers having to worry about passing it through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz-VAPHQjdJR"
   },
   "source": [
    "Let's check to make sure our implementation behaves the same as the original Gemma implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBgBGLSCjqN8"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "fake_token_embedding = pz.nx.nmap(jnp.sin)(\n",
    "    0.333 * (pz.nx.arange(\"seq\", 50) + pz.nx.arange(\"embedding\", 32))\n",
    ")[{\"batch\": np.newaxis, \"heads\": np.newaxis}]\n",
    "fake_token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhzzBhHUjiWP"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "gemma.positional_embeddings.apply_rope(\n",
    "    inputs=fake_token_embedding.unwrap(\"batch\", \"seq\", \"heads\", \"embedding\"),\n",
    "    positions=jnp.arange(50)[None, :],\n",
    "    head_dim=32\n",
    ")[0,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMeyKd2ekyBW"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "handled_rope_layer((\n",
    "    fake_token_embedding,\n",
    "    pz.nx.arange(\"seq\", 50),\n",
    "))[{\"batch\": 0, \"heads\": 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUe1_nXRlA7L"
   },
   "outputs": [],
   "source": [
    "chex.assert_trees_all_close(\n",
    "    gemma.positional_embeddings.apply_rope(\n",
    "        inputs=fake_token_embedding.unwrap(\"batch\", \"seq\", \"heads\", \"embedding\"),\n",
    "        positions=jnp.arange(50)[None, :],\n",
    "        head_dim=32\n",
    "    ),\n",
    "    handled_rope_layer((\n",
    "        fake_token_embedding,\n",
    "        pz.nx.arange(\"seq\", 50),\n",
    "    )).unwrap(\"batch\", \"seq\", \"heads\", \"embedding\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwCNsX7LgpLm"
   },
   "source": [
    "Before we move on, it's worth noting that `WithSideInputsFromInputTuple` isn't magic, and it isn't using any sort of mutable or global state! It's implementation of `__call__` just makes a copy of its child, replaces all of the `HandledSideInputRef`s it owns with `SideInputEffectImpl`s, and calls it:\n",
    "\n",
    "```python\n",
    "@pz.pytree_dataclass\n",
    "class WithSideInputsFromInputTuple(effect_base.EffectHandler):\n",
    "  handler_id: effect_base.HandlerId\n",
    "  body: layer_base.LayerLike\n",
    "  side_input_tags: tuple[Tag, ...]\n",
    "\n",
    "  ...\n",
    "\n",
    "  def __call__(self, argument: tuple[Any, ...]):\n",
    "    inner_arg = argument[0]\n",
    "    side_inputs = argument[1:]\n",
    "    impls = {\n",
    "        tag: SideInputEffectImpl(_value=val, _handler_id=self.handler_id)\n",
    "        for tag, val in zip(self.side_input_tags, side_inputs)\n",
    "    }\n",
    "    handled_body = (\n",
    "        selectors.select(self.body)\n",
    "        .at_instances_of(HandledSideInputRef)\n",
    "        .where(lambda ref: ref.handler_id == self.handler_id)\n",
    "        .apply(lambda ref: impls[ref.tag])\n",
    "    )\n",
    "    return handled_body(inner_arg)\n",
    "```\n",
    "Conceptually, just as JAX operates in terms of function transformations like `jit` or `vmap`, you can think of Penzai as operating in terms of *data structure transformations* like `WithSideInputsFromInputTuple` which rewrite your model's tree structure in a functional way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DFrQKZgf0S6"
   },
   "source": [
    "If you'd like to learn more about Penzai's named axis system or data effects system, check out their dedicated tutorial notebooks: [\"Named Axes in Penzai\"](named_axes.ipynb) and [\"Data Effects\"](data_effects.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYQNIGBNsHu6"
   },
   "source": [
    "### Attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8iNeXgmm-RW"
   },
   "source": [
    "Next up is the attention layer. The logic of this layer [in the Flax implementation](https://github.com/google-deepmind/gemma/blob/f03bf638ae35ecb28a1b31cbe18e3fa628184594/gemma/modules.py#L67) is a bit more complex:\n",
    "\n",
    "```python\n",
    "class Attention(nn.Module):\n",
    "  \"\"\"Attention module.\"\"\"\n",
    "  num_heads: int\n",
    "  num_kv_heads: int\n",
    "  features: int\n",
    "  head_dim: int\n",
    "\n",
    "  @property\n",
    "  def use_qkv_einsum(self):\n",
    "    return self.num_kv_heads == self.num_heads\n",
    "\n",
    "  def setup(self):\n",
    "    self.attn_vec_einsum = layers.Einsum(shape=(self.num_heads, self.head_dim, self.features))\n",
    "\n",
    "    if self.use_qkv_einsum:\n",
    "      self.qkv_einsum = layers.Einsum(shape=(3, self.num_heads, self.features, self.head_dim))\n",
    "    else:\n",
    "      self.q_einsum = layers.Einsum(shape=(self.num_heads, self.features, self.head_dim))\n",
    "      self.kv_einsum = layers.Einsum(shape=(2, self.num_kv_heads, self.features, self.head_dim))\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      x: jax.Array,\n",
    "      segment_pos: jax.Array,\n",
    "      cache: LayerCache | None,\n",
    "      attn_mask: jax.Array,\n",
    "  ) -> tuple[LayerCache | None, jax.Array]:\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    if self.use_qkv_einsum:\n",
    "      query_proj, key_proj, value_proj = self.qkv_einsum('BTD,SNDH->SBTNH', x)\n",
    "    else:\n",
    "      query_proj = self.q_einsum('BTD,NDH->BTNH', x)\n",
    "      key_proj, value_proj = self.kv_einsum('BSD,CKDH->CBSKH', x)\n",
    "\n",
    "    query_proj = positional_embeddings.apply_rope(\n",
    "        query_proj, segment_pos, head_dim=self.head_dim,\n",
    "    )\n",
    "    query_scaled = query_proj * self.head_dim**-0.5\n",
    "    key_proj = positional_embeddings.apply_rope(\n",
    "        key_proj, segment_pos, head_dim=self.head_dim,\n",
    "    )\n",
    "\n",
    "    if not self.use_qkv_einsum:\n",
    "      value_proj = jnp.repeat(value_proj, self.num_heads, axis=-2)\n",
    "      key_proj = jnp.repeat(key_proj, self.num_heads, axis=-2)\n",
    "\n",
    "    if cache is not None:\n",
    "      end_index = cache['end_index'][0]\n",
    "      slice_indices = (0, end_index % cache['v'].shape[1], 0, 0)\n",
    "      value_proj = jax.lax.dynamic_update_slice(\n",
    "          cache['v'], value_proj, slice_indices,\n",
    "      )\n",
    "      key_proj = jax.lax.dynamic_update_slice(\n",
    "          cache['k'], key_proj, slice_indices\n",
    "      )\n",
    "\n",
    "    logits = jnp.einsum('BTNH,BSNH->BTNS', query_scaled, key_proj)\n",
    "    padded_logits = jnp.where((jnp.expand_dims(attn_mask, -2)), logits, K_MASK)\n",
    "    probs = jax.nn.softmax(padded_logits, axis=-1).astype(key_proj.dtype)\n",
    "    encoded = jnp.einsum('BTNS,BSNH->BTNH', probs, value_proj)\n",
    "    attn_output = self.attn_vec_einsum('BTNH,NHD->BTD', encoded)\n",
    "\n",
    "    if cache is not None:\n",
    "      new_cache = {'v': value_proj, 'k': key_proj, 'end_index': cache['end_index'] + seq_len}\n",
    "    else:\n",
    "      new_cache = None\n",
    "\n",
    "    return new_cache, attn_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_G8ZyUbn43v"
   },
   "source": [
    "This module is doing a lot of different things:\n",
    "- Depending on whether `cache` is provided, it either updates a key-value cache and returns it, or just runs without caching.\n",
    "  - This particular reference implementation of Gemma does not use Flax's own mutable state primitives, but some other implementations do, e.g. [the one in MaxText](https://github.com/google/maxtext/blob/47d8e065175a404ce35d43af6a2f378aeb7d1bac/MaxText/layers/attentions.py#L459).\n",
    "- Depending on the number of heads, it either computes queries, keys, and values in one call, or separately computes queries and keys/values.\n",
    "- The module takes four arguments, of which one (`x`) is an input from the previous layer, two (`segment_pos` and `attn_mask`) are common across all attention layers, and one (`cache`) is a state argument.\n",
    "- The module returns two arguments, one of which (`attn_output`) is intended to be passed onward, and one of which (`new_cache`) needs to be threaded out as an updated state.\n",
    "\n",
    "In Penzai, by convention every layer does exactly one thing, and the computation is directly mirrored in the model's structure. Instead of inferring different computation paths at runtime based on the arguments or configuration arguments, idiomatic Penzai models instead use *different classes* to represent substantially different computation paths.\n",
    "\n",
    "To create a Penzai version of this attention layer:\n",
    "\n",
    "- We'll specialize our implementation to always compute queries, keys, and values with three separate matrix multiplies.\n",
    "  - This might have a slight performance penalty, but it will make it easier to patch the model.\n",
    "- Instead of repeating the keys and values when `num_kv_heads` is 1, we'll simply omit the heads axis and allow it to broadcast automatically. (Note that the only valid values of `num_kv_heads` in the Flax implementation are `num_heads` and 1.)\n",
    "- We'll assume there is no key-value cache for now. In Section 3 we'll define a different adaptation of the attention layer that does have a KV cache. (In idiomatic Penzai models, different runtime behaviors are represented by defining different classes.)\n",
    "- We'll omit the `segment_pos` argument entirely. Since our positional embeddings already receive the token positions as a side input, we don't have to thread them through the attention layer.\n",
    "- We'll treat the `attn_mask` argument as a side input, since it will be shared across all the attention blocks.\n",
    "- We'll refactor the overall computation to decompose it into logically-distinct components like we did for the FeedForward block:\n",
    "  - The outermost component is in charge of routing the arrays between the query, key, value, attention, and output computations.\n",
    "  - The innermost components each do a single thing, e.g. running a single tensor contraction, applying an attention mask, or taking a softmax.\n",
    "\n",
    "For convenience, we'll also collect the configuration arguments into a dataclass. This `GemmaTransformerConfig` will just be used to simplify passing arguments during construction of the model; unlike in the Flax version, it won't actually be part of the resulting model.\n",
    "\n",
    "Here's a general implementation which computes queries, keys, and values separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mA_vGq7orrVi"
   },
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class GemmaTransformerConfig:\n",
    "  # Main configuration:\n",
    "  num_heads: int\n",
    "  embedding_dim: int\n",
    "  projection_dim: int\n",
    "  single_kv_head: bool\n",
    "  mlp_hidden_dim: int\n",
    "  num_decoder_blocks: int\n",
    "  vocab_size: int\n",
    "  dtype: jax.typing.DTypeLike\n",
    "\n",
    "pz_gemma_config = GemmaTransformerConfig(\n",
    "    num_heads=flax_gemma_config.num_heads,\n",
    "    embedding_dim=flax_gemma_config.embed_dim,\n",
    "    projection_dim=flax_gemma_config.embed_dim // flax_gemma_config.num_heads,\n",
    "    single_kv_head=(flax_gemma_config.num_kv_heads == 1),\n",
    "    mlp_hidden_dim=flax_gemma_config.hidden_dim,\n",
    "    num_decoder_blocks=flax_gemma_config.num_layers,\n",
    "    vocab_size=flax_gemma_config.num_embed,\n",
    "    dtype=jnp.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBkVYfFtsIou"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class ApplyAttentionMask(pz.Layer):\n",
    "  mask: pz.de.SideInputEffect[pz.nx.NamedArray]\n",
    "  masked_out_value: jax.typing.ArrayLike\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(\n",
    "      cls,\n",
    "      mask_tag: Any,\n",
    "      masked_out_value: jax.typing.ArrayLike = -2.3819763e38,\n",
    "  ) -> 'ApplyAttentionMask':\n",
    "    return cls(\n",
    "        mask=pz.de.SideInputRequest(tag=mask_tag),\n",
    "        masked_out_value=masked_out_value,\n",
    "    )\n",
    "\n",
    "  def __call__(self, x: pz.nx.NamedArray) -> pz.nx.NamedArray:\n",
    "    return pz.nx.nmap(jnp.where)(self.mask.ask(), x, self.masked_out_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRuLzfF38ctJ"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class GemmaAttention(pz.Layer):\n",
    "  input_to_query: pz.LayerLike\n",
    "  input_to_key: pz.LayerLike\n",
    "  input_to_value: pz.LayerLike\n",
    "  query_key_to_attn: pz.LayerLike\n",
    "  attn_value_to_output: pz.LayerLike\n",
    "\n",
    "  def __call__(self, x: pz.nx.NamedArray) -> pz.nx.NamedArray:\n",
    "    query = self.input_to_query(x)\n",
    "    key = self.input_to_key(x)\n",
    "    value = self.input_to_value(x)\n",
    "    attn = self.query_key_to_attn((query, key))\n",
    "    output = self.attn_value_to_output((attn, value))\n",
    "    return output\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config: GemmaTransformerConfig) -> \"GemmaAttention\":\n",
    "    num_heads = config.num_heads\n",
    "    embedding_dim = config.embedding_dim\n",
    "    projection_dim = config.projection_dim\n",
    "    single_kv_head = config.single_kv_head\n",
    "\n",
    "    if single_kv_head:\n",
    "      kv_output_axes = {\"projection\": projection_dim}\n",
    "      kv_einsum_heads = {}\n",
    "    else:\n",
    "      kv_output_axes = {\"heads\": num_heads, \"projection\": projection_dim}\n",
    "      kv_einsum_heads = {\"heads\": \"h\"}\n",
    "\n",
    "    return cls(\n",
    "        input_to_query=pz.nn.Sequential([\n",
    "            pz.nn.add_parameter_prefix(\n",
    "                \"query\",\n",
    "                pz.nn.Linear.from_config(\n",
    "                    input_axes={\"embedding\": embedding_dim},\n",
    "                    output_axes={\n",
    "                        \"heads\": num_heads,\n",
    "                        \"projection\": projection_dim,\n",
    "                    },\n",
    "                    dtype=config.dtype,\n",
    "                ),\n",
    "            ),\n",
    "            ApplyRoPE.from_config(\n",
    "                positions_tag=\"token_positions\",\n",
    "                embedding_axis=\"projection\",\n",
    "            ),\n",
    "            pz.nn.ConstantRescale(by=(projection_dim**-0.5)),\n",
    "        ]),\n",
    "        input_to_key=pz.nn.Sequential([\n",
    "            pz.nn.add_parameter_prefix(\n",
    "                \"key\",\n",
    "                pz.nn.Linear.from_config(\n",
    "                    input_axes={\"embedding\": embedding_dim},\n",
    "                    output_axes=kv_output_axes,\n",
    "                    dtype=config.dtype,\n",
    "                ),\n",
    "            ),\n",
    "            ApplyRoPE.from_config(\n",
    "                positions_tag=\"token_positions\",\n",
    "                embedding_axis=\"projection\",\n",
    "            ),\n",
    "        ]),\n",
    "        input_to_value=pz.nn.Sequential([\n",
    "            pz.nn.add_parameter_prefix(\n",
    "                \"value\",\n",
    "                pz.nn.Linear.from_config(\n",
    "                    input_axes={\"embedding\": embedding_dim},\n",
    "                    output_axes=kv_output_axes,\n",
    "                    dtype=config.dtype,\n",
    "                ),\n",
    "            ),\n",
    "        ]),\n",
    "        query_key_to_attn=pz.nn.Sequential([\n",
    "            pz.nn.NamedEinsum(\n",
    "                (\n",
    "                    {\"seq\": \"tq\", \"heads\": \"h\", \"projection\": \"p\"},\n",
    "                    {\"seq\": \"tkv\", **kv_einsum_heads, \"projection\": \"p\"},\n",
    "                ),\n",
    "                {\"seq\": \"tq\", \"heads\": \"h\", \"kv_seq\": \"tkv\"}\n",
    "            ),\n",
    "            ApplyAttentionMask.from_config(mask_tag=\"attn_mask\"),\n",
    "            pz.nn.Softmax(\"kv_seq\"),\n",
    "        ]),\n",
    "        attn_value_to_output=pz.nn.Sequential([\n",
    "            pz.nn.NamedEinsum(\n",
    "                (\n",
    "                    {\"seq\": \"tq\", \"heads\": \"h\", \"kv_seq\": \"tkv\"},\n",
    "                    {\"seq\": \"tkv\", **kv_einsum_heads, \"projection\": \"p\"},\n",
    "                ),\n",
    "                {\"seq\": \"tq\", \"heads\": \"h\", \"projection\": \"p\"}\n",
    "            ),\n",
    "            pz.nn.add_parameter_prefix(\n",
    "                \"output\",\n",
    "                pz.nn.Linear.from_config(\n",
    "                    input_axes={\n",
    "                        \"heads\": num_heads,\n",
    "                        \"projection\": projection_dim,\n",
    "                    },\n",
    "                    output_axes={\"embedding\": embedding_dim},\n",
    "                    dtype=config.dtype,\n",
    "                ),\n",
    "            ),\n",
    "        ]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2hzqMW0t9Jt"
   },
   "source": [
    "And this is what it looks like when we construct it and migrate over the parameters from the Flax version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02ZTUuAuuDaW"
   },
   "outputs": [],
   "source": [
    "attn_param_mapping = {\n",
    "    \"query.weights\": pz.nx.NamedArray.wrap(\n",
    "        params['transformer']['layer_0']['attn']['q_einsum']['w']\n",
    "    ).tag(\"heads\", \"embedding\", \"projection\"),\n",
    "    \"key.weights\": pz.nx.NamedArray.wrap(\n",
    "        params['transformer']['layer_0']['attn']['kv_einsum']['w'][0,0]\n",
    "    ).tag(\"embedding\", \"projection\"),\n",
    "    \"value.weights\": pz.nx.NamedArray.wrap(\n",
    "        params['transformer']['layer_0']['attn']['kv_einsum']['w'][1,0]\n",
    "    ).tag(\"embedding\", \"projection\"),\n",
    "    \"output.weights\": pz.nx.NamedArray.wrap(\n",
    "        params['transformer']['layer_0']['attn']['attn_vec_einsum']['w']\n",
    "    ).tag(\"heads\", \"projection\", \"embedding\"),\n",
    "}\n",
    "attn_def = GemmaAttention.from_config(pz_gemma_config)\n",
    "attn_layer = (\n",
    "    attn_def.select()\n",
    "    .at_instances_of(pz.nn.UninitializedParameter)\n",
    "    .apply(lambda param: param.initialize_with_value(\n",
    "        attn_param_mapping[param.name], strict_dtype=False,\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZH48KrH5R3DZ"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "attn_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPI_qEkLuCe7"
   },
   "source": [
    "Let's run it to make sure the outputs are the same. We'll again use `WithSideInputFromArg` to provide the necessary side inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPvlJePbuHg9"
   },
   "outputs": [],
   "source": [
    "captured_attn = isolate_submodel.call_and_extract_submodel(\n",
    "    pz.select(intercepted_gemma).at(\n",
    "        (lambda root: root.body.submodule_calls[(1, 'layer_0.__call__')].submodule_calls[(1, 'attn.__call__')])\n",
    "    ),\n",
    "    example_gemma_wrapped_arg\n",
    ")\n",
    "saved_input_embedding = pz.nx.wrap(captured_attn.saved_input.args[0]).tag(\"batch\", \"seq\", \"embedding\")\n",
    "saved_positions = pz.nx.wrap(captured_attn.saved_input.args[1]).tag(\"batch\", \"seq\")\n",
    "saved_attn_mask = pz.nx.wrap(captured_attn.saved_input.args[3]).tag(\"batch\", \"seq\", \"kv_seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q61AN3P-THcF"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "wrapped = pz.de.WithSideInputsFromInputTuple.handling(\n",
    "    attn_layer, tags=[\"token_positions\", \"attn_mask\"]\n",
    ")\n",
    "wrapped((saved_input_embedding, saved_positions, saved_attn_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJLcnqE7Tr8W"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "captured_attn.saved_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z72PIJhWUpUK"
   },
   "outputs": [],
   "source": [
    "chex.assert_trees_all_close(\n",
    "    wrapped((saved_input_embedding, saved_positions, saved_attn_mask)).unwrap(\"batch\", \"seq\", \"embedding\"),\n",
    "    captured_attn.saved_output[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQbo9qtWuK-M"
   },
   "source": [
    "In Section 1, we discussed how it is difficult to inspect the attention pattern in the Flax implementation, since the attention computation isn't exposed. In contrast, it's trivial to inspect it in our Penzai model, since the computation of the attention weights is its own layer. In fact, here's all of the intermediate activations throughout the entire attention computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xWFOmvyVN0d"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "wrapped_with_intermediates = interleave_intermediates.run_and_interleave_intermediates(\n",
    "    wrapped,\n",
    "    (saved_input_embedding, saved_positions, saved_attn_mask)\n",
    ")\n",
    "(\n",
    "    wrapped_with_intermediates.select()\n",
    "    .at_instances_of(GemmaAttention)\n",
    "    .at_instances_of(interleave_intermediates.IdentityWithSavedActivations)\n",
    "    .at_instances_of(pz.nx.NamedArray)\n",
    "    .show_value()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnYRqvoRW7-S"
   },
   "source": [
    "We can pull out the attention mask and visualize it in full resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfIeRKl4XBmm"
   },
   "outputs": [],
   "source": [
    "# Copied from the above printout by clicking the \"copy\" symbol after the array visualization:\n",
    "path_fn = (lambda root: root.sublayers[1].body.sublayers[1].query_key_to_attn.sublayers[6].saved_activations[0])\n",
    "saved_attention_pattern = path_fn(wrapped_with_intermediates)\n",
    "pz.ts.render_array(\n",
    "    # Using the copied function to pull out the value of the array we clicked:\n",
    "    saved_attention_pattern,\n",
    "    truncate=False,\n",
    "    # This adds the actual token values to the hover tooltips:\n",
    "    axis_item_labels={\n",
    "        \"seq\": [repr(vocab.IdToPiece(int(t))) for t in tokens],\n",
    "        \"kv_seq\": [repr(vocab.IdToPiece(int(t))) for t in tokens],\n",
    "    },\n",
    "    # This overlays the attention mask to hide masked-out locations:\n",
    "    valid_mask=saved_attn_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUoF6IhVX-_S"
   },
   "source": [
    "By default, treescope tries to emphasize detail by adjusting the colormap so that three standard deviations are visible, and outliers are truncated (shown as \"+\"). But you can extend the colormap range upward by clicking on any \"+\" cell, and you can also switch to a symmetric-logarithm colormap by clicking on cells with values near zero. Try seeing what attention patterns you can spot! (Clicking on cells also moves the hover tooltip that you clicked below the visualization, so you can copy it later if you want.)\n",
    "\n",
    "See the \"induction heads\" notebook for more discussion of looking at attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4sDYyQ4kVFK"
   },
   "source": [
    "### Root-mean-squared layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZzmRaUDkY9R"
   },
   "source": [
    "Gemma uses RMSNorm ([Zhang & Sennrich, 2019](https://arxiv.org/abs/1910.07467)) to normalize the inputs and outputs of the attention block. The Flax implementation defines it [like this](https://github.com/google-deepmind/gemma/blob/c6bd156c246530e1620a7c62de98542a377e3934/gemma/layers.py#L31):\n",
    "```python\n",
    "class RMSNorm(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    scale = self.param('scale', nn.initializers.zeros_init(), (x.shape[-1]))\n",
    "    var = jnp.mean(jnp.square(x), axis=-1, keepdims=True)\n",
    "    normed_inputs = jnp.asarray(x * jnp.reciprocal(jnp.sqrt(var + 1e-06)))\n",
    "    normed_inputs = normed_inputs * (1 + scale)\n",
    "    return normed_inputs\n",
    "```\n",
    "For our Penzai version, we'll make a few minor changes:\n",
    "\n",
    "- We'll separate out the normalization logic from the scaling logic, since they don't depend on each other.\n",
    "- We'll fold the `(1 + scale)` into `scale` so that the scaling logic can be expressed with `pz.nn.Linear`.\n",
    "- We'll re-write the computation to operate over a named axis instead of using the last positional axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lZvIG19lq5w"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class RMSStandardize(pz.Layer):\n",
    "  across: str | tuple[str, ...] = field(metadata={\"pytree_node\": False})\n",
    "  epsilon: float | jax.Array = 1e-6\n",
    "\n",
    "  @pz.checked_layer_call\n",
    "  def __call__(self, value: pz.nx.NamedArray) -> pz.nx.NamedArray:\n",
    "    across = (self.across,) if isinstance(self.across, str) else self.across\n",
    "\n",
    "    @pz.nx.nmap\n",
    "    def _rms_standardize(x):\n",
    "      var = jnp.mean(jnp.square(x))\n",
    "      return x * jnp.reciprocal(jnp.sqrt(var + self.epsilon))\n",
    "\n",
    "    return _rms_standardize(value.untag(*across)).tag(*across)\n",
    "\n",
    "  def input_structure(self) -> Any:\n",
    "    across = (self.across,) if isinstance(self.across, str) else self.across\n",
    "    return pz.chk.ArraySpec.floating_named({\n",
    "        **pz.chk.var(\"B\"),\n",
    "        **pz.chk.vars_for_axes(\"across\", across),\n",
    "    })\n",
    "\n",
    "  def output_structure(self) -> Any:\n",
    "    return self.input_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J93aTtAomWOj"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass(has_implicitly_inherited_fields=True)\n",
    "class RMSLayerNorm(pz.nn.Sequential):\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(\n",
    "      cls,\n",
    "      across_axes: dict[str, int],\n",
    "      epsilon: float | jax.Array = 1e-6,\n",
    "      dtype: jax.typing.DTypeLike = jnp.float32,\n",
    "  ) -> RMSLayerNorm:\n",
    "    return cls([\n",
    "        RMSStandardize(across=tuple(across_axes.keys()), epsilon=epsilon),\n",
    "        pz.nn.add_parameter_prefix(\n",
    "            \"scale\",\n",
    "            pz.nn.Linear.from_config(\n",
    "                input_axes={},\n",
    "                output_axes={},\n",
    "                parallel_axes=across_axes,\n",
    "                initializer=pz.nn.constant_initializer(1.0),\n",
    "                dtype=dtype,\n",
    "            ),\n",
    "        ),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YROhL8HumzNB"
   },
   "source": [
    "Checking for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bvuHv_smySZ"
   },
   "outputs": [],
   "source": [
    "rmsnorm_param_mapping = {\n",
    "    \"scale.weights\": pz.nx.NamedArray.wrap(\n",
    "        1 + params['transformer']['layer_0']['pre_attention_norm']['scale']\n",
    "    ).tag(\"embedding\"),\n",
    "}\n",
    "rmsnorm_def = RMSLayerNorm.from_config(\n",
    "    {\"embedding\": flax_gemma_config.embed_dim}\n",
    ")\n",
    "rmsnorm_layer = (\n",
    "    rmsnorm_def.select()\n",
    "    .at_instances_of(pz.nn.UninitializedParameter)\n",
    "    .apply(lambda param: param.initialize_with_value(\n",
    "        rmsnorm_param_mapping[param.name], strict_dtype=False,\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCcrM2rjnkFR"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "rmsnorm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jomdlabsn1a4"
   },
   "outputs": [],
   "source": [
    "captured_rmsnorm = isolate_submodel.call_and_extract_submodel(\n",
    "    pz.select(intercepted_gemma).at(\n",
    "        (lambda root: root.body.submodule_calls[(1, 'layer_0.__call__')].submodule_calls[(0, 'pre_attention_norm.__call__')])\n",
    "    ),\n",
    "    example_gemma_wrapped_arg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW1TPjRBnsuV"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "rmsnorm_layer(\n",
    "    pz.nx.wrap(captured_rmsnorm.saved_input.args[0])\n",
    "    .tag(\"batch\", \"seq\", \"embedding\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cmr7au4KoSIK"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "captured_rmsnorm.saved_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdWe_iU9oVJv"
   },
   "outputs": [],
   "source": [
    "chex.assert_trees_all_close(\n",
    "    rmsnorm_layer(\n",
    "        pz.nx.wrap(captured_rmsnorm.saved_input.args[0])\n",
    "        .tag(\"batch\", \"seq\", \"embedding\")\n",
    "    ).unwrap(\"batch\", \"seq\", \"embedding\"),\n",
    "    captured_rmsnorm.saved_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn-HZ6lisI8b"
   },
   "source": [
    "### Transformer block layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15OET8aisMGs"
   },
   "source": [
    "Now we can put these pieces together to build the [main transformer block](https://github.com/google-deepmind/gemma/blob/c6bd156c246530e1620a7c62de98542a377e3934/gemma/modules.py#L188):\n",
    "```python\n",
    "class Struct(nn.Module):\n",
    "  \"\"\"Transformer block.\"\"\"\n",
    "\n",
    "  num_heads: int\n",
    "  num_kv_heads: int\n",
    "  embed_dim: int\n",
    "  head_dim: int\n",
    "  hidden_dim: int\n",
    "\n",
    "  def setup(self):\n",
    "    self.pre_attention_norm = layers.RMSNorm()\n",
    "    self.attn = Attention(\n",
    "        num_heads=self.num_heads,\n",
    "        features=self.embed_dim,\n",
    "        head_dim=self.head_dim,\n",
    "        num_kv_heads=self.num_kv_heads,\n",
    "    )\n",
    "    self.pre_ffw_norm = layers.RMSNorm()\n",
    "    self.mlp = FeedForward(features=self.embed_dim, hidden_dim=self.hidden_dim)\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      x: jax.Array,\n",
    "      segment_pos: jax.Array,\n",
    "      cache: LayerCache | None,\n",
    "      attn_mask: jax.Array,\n",
    "  ) -> tuple[LayerCache | None, jax.Array]:\n",
    "    inputs_normalized = self.pre_attention_norm(x)\n",
    "    cache, attn_output = self.attn(\n",
    "        inputs_normalized,\n",
    "        segment_pos,\n",
    "        cache,\n",
    "        attn_mask,\n",
    "    )\n",
    "    attn_output += x\n",
    "    residual = attn_output\n",
    "    attn_output = self.pre_ffw_norm(attn_output)\n",
    "    outputs = self.mlp(attn_output)\n",
    "    outputs = residual + outputs\n",
    "    return cache, outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3d9WRq0MsLYT"
   },
   "source": [
    "This Flax module is mostly a sequence of operations run one after another, but it also has residual connections and explicitly-threaded state. We'll refactor the residual connections into a `Residual` combinator, and we'll drop the state since this version of the implementation doesn't need it.\n",
    "\n",
    "As with the FeedForward layer, we'll express this logic by creating a subclass of `pz.nn.Sequential`, and adding a class method that builds the sequence of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wy4K264esLAP"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass(has_implicitly_inherited_fields=True)\n",
    "class GemmaTransformerBlock(pz.nn.Sequential):\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config: GemmaTransformerConfig) -> GemmaTransformerBlock:\n",
    "    return cls(\n",
    "        sublayers=[\n",
    "            pz.nn.Residual(\n",
    "                pz.nn.Sequential([\n",
    "                    pz.nn.add_parameter_prefix(\n",
    "                        \"pre_attention_norm\",\n",
    "                        RMSLayerNorm.from_config(\n",
    "                            {\"embedding\": config.embedding_dim},\n",
    "                            dtype=config.dtype,\n",
    "                        ),\n",
    "                    ),\n",
    "                    pz.nn.add_parameter_prefix(\n",
    "                        \"attn\",\n",
    "                        GemmaAttention.from_config(config),\n",
    "                    ),\n",
    "                ])\n",
    "            ),\n",
    "            pz.nn.Residual(\n",
    "                pz.nn.Sequential([\n",
    "                    pz.nn.add_parameter_prefix(\n",
    "                        \"pre_ffw_norm\",\n",
    "                        RMSLayerNorm.from_config(\n",
    "                            {\"embedding\": config.embedding_dim},\n",
    "                            dtype=config.dtype,\n",
    "                        ),\n",
    "                    ),\n",
    "                    pz.nn.add_parameter_prefix(\n",
    "                        \"mlp\",\n",
    "                        GemmaFeedForward.from_config(\n",
    "                            embedding_dim=config.embedding_dim,\n",
    "                            hidden_dim=config.mlp_hidden_dim,\n",
    "                            dtype=config.dtype,\n",
    "                        ),\n",
    "                    ),\n",
    "                ])\n",
    "            ),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwMRfWbPtn1r"
   },
   "source": [
    "Let's load it from the checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWqoU5SfttIj"
   },
   "outputs": [],
   "source": [
    "tfblock_param_mapping = {\n",
    "    **{f\"attn.{name}\": value for name,value in attn_param_mapping.items()},\n",
    "    **{f\"mlp.{name}\": value for name,value in ff_param_mapping.items()},\n",
    "    \"pre_attention_norm.scale.weights\": pz.nx.NamedArray.wrap(\n",
    "        1 + params['transformer']['layer_0']['pre_attention_norm']['scale']\n",
    "    ).tag(\"embedding\"),\n",
    "    \"pre_ffw_norm.scale.weights\": pz.nx.NamedArray.wrap(\n",
    "        1 + params['transformer']['layer_0']['pre_ffw_norm']['scale']\n",
    "    ).tag(\"embedding\"),\n",
    "}\n",
    "tfblock_def = GemmaTransformerBlock.from_config(pz_gemma_config)\n",
    "tfblock = (\n",
    "    tfblock_def.select()\n",
    "    .at_instances_of(pz.nn.UninitializedParameter)\n",
    "    .apply(lambda param: param.initialize_with_value(\n",
    "        tfblock_param_mapping[param.name], strict_dtype=False,\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3z44XyLttIs"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "tfblock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zel6KFFvvrGc"
   },
   "source": [
    "And make sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-BWBi4wvwGq"
   },
   "outputs": [],
   "source": [
    "captured_tfblock = isolate_submodel.call_and_extract_submodel(\n",
    "    pz.select(intercepted_gemma).at(\n",
    "        (lambda root: root.body.submodule_calls[(1, 'layer_0.__call__')])\n",
    "    ),\n",
    "    example_gemma_wrapped_arg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Hsp3yqovwGr"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "wrapped = pz.de.WithSideInputsFromInputTuple.handling(\n",
    "    tfblock, tags=[\"token_positions\", \"attn_mask\"]\n",
    ")\n",
    "saved_input_embedding = (\n",
    "    pz.nx.wrap(captured_rmsnorm.saved_input.args[0])\n",
    "    .tag(\"batch\", \"seq\", \"embedding\")\n",
    ")\n",
    "wrapped((saved_input_embedding, saved_positions, saved_attn_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilf5Z6RTvwGr"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "captured_tfblock.saved_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u6Ogj7evwGr"
   },
   "outputs": [],
   "source": [
    "chex.assert_trees_all_close(\n",
    "    wrapped(\n",
    "        (saved_input_embedding, saved_positions, saved_attn_mask)\n",
    "    ).unwrap(\"batch\", \"seq\", \"embedding\"),\n",
    "    captured_tfblock.saved_output[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0h1kBE9fdCS"
   },
   "source": [
    "### Token embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp1M3c_pfzki"
   },
   "source": [
    "Our last subcomponent is the embedding layer, which is responsible for mapping token IDs to vectors and mapping vectors back to (distributions over) token IDs. In Flax, [Gemma's embedding layer](https://github.com/google-deepmind/gemma/blob/f03bf638ae35ecb28a1b31cbe18e3fa628184594/gemma/modules.py#L67) is defined as:\n",
    "```python\n",
    "class Embedder(nn.Module):\n",
    "  \"\"\"Embedder module.\"\"\"\n",
    "  vocab_size: int\n",
    "  embed_dim: int\n",
    "\n",
    "  def setup(self):\n",
    "    self.input_embedding_table = self.param(\n",
    "        'input_embedding',\n",
    "        nn.initializers.normal(),\n",
    "        (self.vocab_size, self.embed_dim),\n",
    "    )\n",
    "\n",
    "  def encode(self, x: jax.Array) -> jax.Array:\n",
    "    x = self.input_embedding_table[(x,)]\n",
    "    x *= jnp.sqrt(self.embed_dim).astype(x.dtype)\n",
    "    return x\n",
    "\n",
    "  def decode(self, x: jax.Array) -> jax.Array:\n",
    "    return jnp.dot(x, self.input_embedding_table.T)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i64pylko91F"
   },
   "source": [
    "Unlike the other layers we've seen so far, this Flax module has multiple methods. This is a common way to express parameter sharing in Flax, but it's not a common pattern in Penzai, because it's hard to represent shared object identity inside a declarative what-you-see-is-what-you-get tree.\n",
    "\n",
    "To port this to an idiomatic Penzai layer:\n",
    "\n",
    "- We'll split up the single `Embedder` module into three Penzai classes:\n",
    "  - The first, `EmbeddingTable`, will be a simple data structure in charge of owning the parameters and configuration.\n",
    "  - The second and third will implement the encoding and decoding steps.\n",
    "  We do this because, by convention, each Penzai layer does a single thing and takes a single input. (This makes it easier to chain multiple layers together and patch their logic.)\n",
    "  - Instead of expressing parameter sharing by calling multiple methods on the same Python object, we'll use the `SideInputEffect` to share the embedding table between separate encoding and decoding layer objects. The main difference from our previous uses of this effect is that the shared value will be stored in the model itself rather than being provided as an argument.\n",
    "- We'll make the embedding table an explicit dataclass attribute, rather than adding it in a `setup` method. In Penzai, classes always have exactly the attributes that they are declared to have.\n",
    "- We'll rewrite the configuration attributes and `setup` logic to live in a classmethod `EmbeddingTable.from_config`. By convention, initialization logic is usually defined inside a classmethod, to avoid changing the automatic dataclass `__init__` method and to make it easier to bypass the initializer if needed.\n",
    "- We'll factor out the `jnp.sqrt(self.embed_dim)` in `encode` into it's own `ConstantRescale` layer, so that it's not tightly coupled to the embedding lookup operation. (Note that different transformer implementations differ on where they put this; some implementations scale up the embedding weights and then divide by `jnp.sqrt(self.embed_dim)` before decoding instead.)\n",
    "- We'll use Penzai's named axis system to give the vocabulary and embedding dimensions informative names.\n",
    "\n",
    "This leads us to the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H1iMoPNgl_B"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class EmbeddingTable(pz.Struct):  # <- Struct is the base type of most Penzai pytree_dataclasses.\n",
    "  embeddings: pz.nn.ParameterLike[pz.nx.NamedArray]\n",
    "  vocabulary_axis: str = dataclasses.field(metadata={\"pytree_node\": False})\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(\n",
    "      cls,\n",
    "      vocab_size: int,\n",
    "      embedding_axes: dict[str, int],\n",
    "      vocabulary_axis: str = \"vocabulary\",\n",
    "      initializer: pz.nn.LinearOperatorWeightInitializer = (\n",
    "          functools.partial(\n",
    "              pz.nn.variance_scaling_initializer,\n",
    "              scale=1.0, mode=\"fan_out\", distribution=\"normal\",\n",
    "          )\n",
    "      ),\n",
    "      dtype: np.typing.DTypeLike = np.float32,\n",
    "  ) -> EmbeddingTable:\n",
    "    if vocabulary_axis in embedding_axes:\n",
    "      raise ValueError(\n",
    "          f\"`vocabulary_axis` {vocabulary_axis} should not appear in\"\n",
    "          f\" `embedding_axes` {embedding_axes}\"\n",
    "      )\n",
    "\n",
    "    return cls(\n",
    "        embeddings=pz.nn.UninitializedParameter(\n",
    "            initializer=functools.partial(\n",
    "                initializer,\n",
    "                input_axes={},\n",
    "                output_axes=embedding_axes,\n",
    "                parallel_axes={vocabulary_axis: vocab_size},\n",
    "                convolution_spatial_axes={},\n",
    "                dtype=dtype,\n",
    "            ),\n",
    "            name=\"embeddings\",\n",
    "        ),\n",
    "        vocabulary_axis=vocabulary_axis,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HVuSmhEx-FN"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class EmbeddingLookup(pz.Layer):\n",
    "  table: EmbeddingTable\n",
    "\n",
    "  @pz.checked_layer_call\n",
    "  def __call__(\n",
    "      self, token_index: pz.nx.NamedArray\n",
    "  ) -> pz.nx.NamedArray:\n",
    "    \"\"\"Retrieves tokens from the embedding table.\"\"\"\n",
    "    return self.table.embeddings.value[{self.table.vocabulary_axis: token_index}]\n",
    "\n",
    "  def input_structure(self) -> Any:\n",
    "    return pz.chk.ArraySpec(\n",
    "        named_shape={**pz.chk.var(\"B\")}, dtype=np.integer\n",
    "    )\n",
    "\n",
    "  def output_structure(self) -> Any:\n",
    "    table_structure = self.table.embeddings.value_structure\n",
    "    non_lookup_shape = dict(table_structure.named_shape)\n",
    "    del non_lookup_shape[self.table.vocabulary_axis]\n",
    "    return pz.chk.ArraySpec(\n",
    "        named_shape={**pz.chk.var(\"B\"), **non_lookup_shape},\n",
    "        dtype=table_structure.dtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_NJsLU8yz8T"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class EmbeddingDecode(pz.Layer):\n",
    "  table: EmbeddingTable\n",
    "\n",
    "  @pz.checked_layer_call\n",
    "  def __call__(\n",
    "      self, arg: pz.nx.NamedArray\n",
    "  ) -> pz.nx.NamedArray:\n",
    "    \"\"\"Retrieves tokens from the embedding table.\"\"\"\n",
    "    contracting_axes = [\n",
    "        name for name in self.table.embeddings.value.named_shape.keys()\n",
    "        if name != self.table.vocabulary_axis\n",
    "    ]\n",
    "    return pz.nn.contract(contracting_axes, arg, self.table.embeddings.value)\n",
    "\n",
    "  def input_structure(self) -> Any:\n",
    "    table_shape = dict(\n",
    "        self.table.embeddings.value_structure.named_shape\n",
    "    )\n",
    "    del table_shape[self.table.vocabulary_axis]\n",
    "    return pz.chk.ArraySpec(\n",
    "        named_shape={**pz.chk.var(\"B\"), **table_shape},\n",
    "        dtype=np.floating,\n",
    "    )\n",
    "\n",
    "  def output_structure(self) -> Any:\n",
    "    table_shape = self.table.embeddings.value_structure.named_shape\n",
    "    return pz.chk.ArraySpec(\n",
    "        named_shape={\n",
    "            **pz.chk.var(\"B\"),\n",
    "            self.table.vocabulary_axis: table_shape[self.table.vocabulary_axis]\n",
    "        },\n",
    "        dtype=np.floating\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAgrxWLiAN3a"
   },
   "source": [
    "Let's make sure each of these work properly on their own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rcY4SbXAWdr"
   },
   "outputs": [],
   "source": [
    "emb_table_def = EmbeddingTable.from_config(\n",
    "    vocab_size=flax_gemma_config.num_embed,\n",
    "    embedding_axes={\"embedding\": flax_gemma_config.embed_dim},\n",
    "    dtype=jnp.bfloat16,\n",
    ")\n",
    "emb_table_param_mapping = {\n",
    "    \"embeddings\": pz.nx.NamedArray.wrap(\n",
    "        params['transformer']['embedder']['input_embedding']\n",
    "    ).tag(\"vocabulary\", \"embedding\"),\n",
    "}\n",
    "emb_table = (\n",
    "    emb_table_def.select()\n",
    "    .at_instances_of(pz.nn.UninitializedParameter)\n",
    "    .apply(lambda param: param.initialize_with_value(\n",
    "        emb_table_param_mapping[param.name], strict_dtype=False,\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxlsfxvQv2N4"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "emb_encoder = pz.nn.Sequential([\n",
    "    EmbeddingLookup(emb_table),\n",
    "    pz.nn.ConstantRescale(by=jnp.sqrt(flax_gemma_config.embed_dim).astype(emb_table.embeddings.value.dtype)),\n",
    "])\n",
    "emb_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqzvqHCBwn9x"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "emb_encoder(pz.nx.wrap(tokens).tag(\"seq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZGMz3ZxxkrA"
   },
   "outputs": [],
   "source": [
    "captured_emb_encoder = isolate_submodel.call_and_extract_submodel(\n",
    "    pz.select(intercepted_gemma).at(\n",
    "        (lambda root: root.body.submodule_calls[(0, 'embedder.encode')])\n",
    "    ),\n",
    "    example_gemma_wrapped_arg\n",
    ")\n",
    "chex.assert_trees_all_close(\n",
    "    emb_encoder(\n",
    "        pz.nx.wrap(captured_emb_encoder.saved_input.args[0]).tag(\"batch\", \"seq\")\n",
    "    ).unwrap(\"batch\", \"seq\", \"embedding\"),\n",
    "    captured_emb_encoder.submodel(captured_emb_encoder.saved_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLFHeY3cx9ah"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "emb_decoder = EmbeddingDecode(emb_table)\n",
    "emb_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuLxoGGXyJhD"
   },
   "outputs": [],
   "source": [
    "captured_emb_decoder = isolate_submodel.call_and_extract_submodel(\n",
    "    pz.select(intercepted_gemma).at(\n",
    "        (lambda root: root.body.submodule_calls[(20, 'embedder.decode')])\n",
    "    ),\n",
    "    example_gemma_wrapped_arg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_t8zlUQ6yQih"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "emb_decoder(\n",
    "    pz.nx.wrap(captured_emb_decoder.saved_input.args[0])\n",
    "    .tag(\"batch\", \"seq\", \"embedding\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbJaK-7zycwZ"
   },
   "outputs": [],
   "source": [
    "chex.assert_trees_all_close(\n",
    "    emb_decoder(\n",
    "        pz.nx.wrap(captured_emb_decoder.saved_input.args[0])\n",
    "        .tag(\"batch\", \"seq\", \"embedding\")\n",
    "    ).unwrap(\"batch\", \"seq\", \"vocabulary\"),\n",
    "    captured_emb_decoder.submodel(captured_emb_decoder.saved_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmEA6dpfAWp5"
   },
   "source": [
    "What about parameter sharing? Penzai's parameter utilities\n",
    "assume each parameter in your model PyTree is independent, which means we can't just put the embedding table in the encoding and decoding steps; this wouldn't properly tie their weights.\n",
    "\n",
    "We can express this using the same `SideInputEffect` we used to share the attention mask and RoPE positions. (In fact, we've already briefly seen this when looking at the intercepted Flax model.) Penzai includes a few utilities to help us set this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dy8Z79pDAmNN"
   },
   "outputs": [],
   "source": [
    "# Temporarily mark the initializer as shareable, so we can find it later.\n",
    "shareable_emb_table_def = pz.nn.mark_shareable(\n",
    "    pz.nn.add_parameter_prefix(\"embedder\", emb_table_def)\n",
    ")\n",
    "# Use the same definition (with the same parameter name) twice:\n",
    "encode_then_decode_def = pz.nn.Sequential([\n",
    "    EmbeddingLookup(shareable_emb_table_def),\n",
    "    pz.nn.ConstantRescale(by=jnp.sqrt(flax_gemma_config.embed_dim).astype(jnp.bfloat16)),\n",
    "    HelloWorld(),\n",
    "    EmbeddingDecode(shareable_emb_table_def),\n",
    "])\n",
    "encode_then_decode_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYw_coDW0B8z"
   },
   "outputs": [],
   "source": [
    "# \"Attach\" the shared parameter to a single point in the tree. The shared\n",
    "# parameter will now be \"owned\" by a `WithConstantSideInputs` handler.\n",
    "shared_encode_then_decode_def = pz.nn.attach_shared_parameters(encode_then_decode_def)\n",
    "shared_encode_then_decode_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60QZvnhS187J"
   },
   "outputs": [],
   "source": [
    "# Initialize it as normal:\n",
    "qualified_emb_table_param_mapping = {\n",
    "    \"embedder.embeddings\": pz.nx.NamedArray.wrap(\n",
    "        params['transformer']['embedder']['input_embedding']\n",
    "    ).tag(\"vocabulary\", \"embedding\"),\n",
    "}\n",
    "shared_encode_then_decode = (\n",
    "    shared_encode_then_decode_def.select()\n",
    "    .at_instances_of(pz.nn.UninitializedParameter)\n",
    "    .apply(lambda param: param.initialize_with_value(\n",
    "        qualified_emb_table_param_mapping[param.name], strict_dtype=False,\n",
    "    ))\n",
    ")\n",
    "shared_encode_then_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oinoryWB2FTf"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "shared_encode_then_decode(pz.nx.wrap(tokens).tag(\"seq\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJBa37lwAmeJ"
   },
   "source": [
    "Note that marking parameters as shareable is temporary: these annotations are used by `attach_shared_parameters` and then forgotten. In the final model tree, we can identify the shared parameters because they use `SharedParameterLookup` nodes instead of ordinary parameters.\n",
    "\n",
    "This is an example of another one of Penzai's design principles: layers should make as minimal assumptions as possible about the implementation of their children. In this case, `EmbeddingTable.embeddings` is annotated as having type `ParameterLike`, which means that it can be *any* PyTree-dataclass type that defines `value` and `value_structure` properties. `Parameter` instances store their values there, but `SharedParameterLookup` instances instead redirect `.value` to `.ref.ask()`. This means the `EmbeddingTable` doesn't have to worry about whether its parameter is shared or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STUbNfZv0_ZT"
   },
   "source": [
    "### Putting it together: The top-level Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF_Tt88iw9D3"
   },
   "source": [
    "At last, we can assemble the full model, which runs each of these sublayers in the appropriate order! The [Flax implementation](https://github.com/google-deepmind/gemma/blob/c6bd156c246530e1620a7c62de98542a377e3934/gemma/transformer.py#L136) is pretty straighforward, so we'll skip looking at it and dive right into the Penzai version.\n",
    "\n",
    "Since the transformer object is intended to be the top-level module, we'll have its `__call__` take a structure of inputs and handle the unpacking of it. We'll still make it a `pz.Layer`, though, so that it composes with the other Penzai utilities that assume `__call__` takes one argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3_sVirQ1C20"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class GemmaInputs(pz.Struct):\n",
    "  tokens: pz.nx.NamedArray\n",
    "  positions: pz.nx.NamedArray\n",
    "  attention_mask: pz.nx.NamedArray\n",
    "\n",
    "\n",
    "@pz.pytree_dataclass\n",
    "class GemmaTransformer(pz.Layer):\n",
    "  config: GemmaTransformerConfig = field(metadata={\"pytree_node\": False})\n",
    "  body: pz.LayerLike\n",
    "\n",
    "  def __call__(self, inputs: GemmaInputs) -> pz.nx.NamedArray:\n",
    "    return self.body((inputs.tokens, inputs.positions, inputs.attention_mask))\n",
    "\n",
    "  @classmethod\n",
    "  def from_config(cls, config: GemmaTransformerConfig) -> GemmaTransformer:\n",
    "    emb_table = pz.nn.mark_shareable(\n",
    "        pz.nn.add_parameter_prefix(\n",
    "            \"embedder\",\n",
    "            EmbeddingTable.from_config(\n",
    "                vocab_size=config.vocab_size,\n",
    "                embedding_axes={\"embedding\": config.embedding_dim},\n",
    "                dtype=config.dtype,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    sublayers = []\n",
    "    sublayers.extend([\n",
    "        EmbeddingLookup(emb_table),\n",
    "        pz.nn.ConstantRescale(\n",
    "            by=jnp.sqrt(config.embedding_dim).astype(config.dtype)\n",
    "        ),\n",
    "    ])\n",
    "    for i in range(config.num_decoder_blocks):\n",
    "      sublayers.append(\n",
    "          pz.nn.add_parameter_prefix(\n",
    "              f\"block_{i}\", GemmaTransformerBlock.from_config(config)\n",
    "          )\n",
    "      )\n",
    "    sublayers.extend([\n",
    "        pz.nn.add_parameter_prefix(\n",
    "            \"final_norm\",\n",
    "            RMSLayerNorm.from_config(\n",
    "                across_axes={\"embedding\": config.embedding_dim},\n",
    "                dtype=config.dtype,\n",
    "            ),\n",
    "        ),\n",
    "        EmbeddingDecode(emb_table),\n",
    "    ])\n",
    "    return GemmaTransformer(\n",
    "        config=config,\n",
    "        body=pz.de.WithSideInputsFromInputTuple.handling(\n",
    "            pz.nn.attach_shared_parameters(pz.nn.Sequential(sublayers)),\n",
    "            tags=[\"token_positions\", \"attn_mask\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  @classmethod\n",
    "  def from_pretrained(cls, flax_params: dict[str, Any]) -> GemmaTransformer:\n",
    "    flax_gemma_config = gemma.transformer.TransformerConfig.from_params(\n",
    "        flax_params\n",
    "    )\n",
    "    config = GemmaTransformerConfig(\n",
    "        num_heads=flax_gemma_config.num_heads,\n",
    "        embedding_dim=flax_gemma_config.embed_dim,\n",
    "        projection_dim=flax_gemma_config.head_dim,\n",
    "        single_kv_head=(flax_gemma_config.num_kv_heads == 1),\n",
    "        mlp_hidden_dim=flax_gemma_config.hidden_dim,\n",
    "        num_decoder_blocks=flax_gemma_config.num_layers,\n",
    "        vocab_size=flax_gemma_config.num_embed,\n",
    "        dtype=flax_params[\"transformer\"][\"embedder\"][\"input_embedding\"].dtype,\n",
    "    )\n",
    "    model_def = cls.from_config(config)\n",
    "    ftp = flax_params[\"transformer\"]\n",
    "    parameter_mapping = {\n",
    "        \"embedder.embeddings\": pz.nx.NamedArray.wrap(\n",
    "            ftp[\"embedder\"][\"input_embedding\"]\n",
    "        ).tag(\"vocabulary\", \"embedding\"),\n",
    "        \"final_norm.scale.weights\": pz.nx.NamedArray.wrap(\n",
    "            1 + ftp[\"final_norm\"][\"scale\"]\n",
    "        ).tag(\"embedding\"),\n",
    "    }\n",
    "    for i in range(config.num_decoder_blocks):\n",
    "      parameter_mapping.update({\n",
    "          f\"block_{i}.pre_attention_norm.scale.weights\": pz.nx.NamedArray.wrap(\n",
    "              1 + ftp[f\"layer_{i}\"][\"pre_attention_norm\"][\"scale\"]\n",
    "          ).tag(\"embedding\"),\n",
    "          f\"block_{i}.pre_ffw_norm.scale.weights\": pz.nx.NamedArray.wrap(\n",
    "              1 + ftp[f\"layer_{i}\"][\"pre_ffw_norm\"][\"scale\"]\n",
    "          ).tag(\"embedding\"),\n",
    "          f\"block_{i}.mlp.gating_linear.weights\": pz.nx.NamedArray.wrap(\n",
    "              ftp[f\"layer_{i}\"][\"mlp\"][\"gating_einsum\"][0]\n",
    "          ).tag(\"embedding\", \"neurons\"),\n",
    "          f\"block_{i}.mlp.value_linear.weights\": pz.nx.NamedArray.wrap(\n",
    "              ftp[f\"layer_{i}\"][\"mlp\"][\"gating_einsum\"][1]\n",
    "          ).tag(\"embedding\", \"neurons\"),\n",
    "          f\"block_{i}.mlp.out_linear.weights\": pz.nx.NamedArray.wrap(\n",
    "              ftp[f\"layer_{i}\"][\"mlp\"][\"linear\"]\n",
    "          ).tag(\"neurons\", \"embedding\"),\n",
    "          f\"block_{i}.attn.output.weights\": pz.nx.NamedArray.wrap(\n",
    "              ftp[f\"layer_{i}\"][\"attn\"][\"attn_vec_einsum\"][\"w\"]\n",
    "          ).tag(\"heads\", \"projection\", \"embedding\"),\n",
    "      })\n",
    "      if config.single_kv_head:\n",
    "        parameter_mapping.update({\n",
    "            f\"block_{i}.attn.query.weights\": pz.nx.NamedArray.wrap(\n",
    "                ftp[f\"layer_{i}\"][\"attn\"][\"q_einsum\"][\"w\"]\n",
    "            ).tag(\"heads\", \"embedding\", \"projection\"),\n",
    "            f\"block_{i}.attn.key.weights\": pz.nx.NamedArray.wrap(\n",
    "                ftp[f\"layer_{i}\"][\"attn\"][\"kv_einsum\"][\"w\"][0, 0]\n",
    "            ).tag(\"embedding\", \"projection\"),\n",
    "            f\"block_{i}.attn.value.weights\": pz.nx.NamedArray.wrap(\n",
    "                ftp[f\"layer_{i}\"][\"attn\"][\"kv_einsum\"][\"w\"][1, 0]\n",
    "            ).tag(\"embedding\", \"projection\"),\n",
    "        })\n",
    "      else:\n",
    "        parameter_mapping.update({\n",
    "            f\"block_{i}.attn.query.weights\": pz.nx.NamedArray.wrap(\n",
    "                ftp[f\"layer_{i}\"][\"attn\"][\"qkv_einsum\"][\"w\"][0]\n",
    "            ).tag(\"heads\", \"embedding\", \"projection\"),\n",
    "            f\"block_{i}.attn.key.weights\": pz.nx.NamedArray.wrap(\n",
    "                ftp[f\"layer_{i}\"][\"attn\"][\"qkv_einsum\"][\"w\"][1]\n",
    "            ).tag(\"heads\", \"embedding\", \"projection\"),\n",
    "            f\"block_{i}.attn.value.weights\": pz.nx.NamedArray.wrap(\n",
    "                ftp[f\"layer_{i}\"][\"attn\"][\"qkv_einsum\"][\"w\"][2]\n",
    "            ).tag(\"heads\", \"embedding\", \"projection\"),\n",
    "        })\n",
    "    return (\n",
    "        model_def.select()\n",
    "        .at_instances_of(pz.nn.UninitializedParameter)\n",
    "        .apply(\n",
    "            lambda param: param.initialize_with_value(\n",
    "                parameter_mapping[param.name],\n",
    "                strict_dtype=False,\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYepsqmLXjzc"
   },
   "outputs": [],
   "source": [
    "pz_gemma_model = GemmaTransformer.from_pretrained(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPdqUz-SaYyW"
   },
   "source": [
    "We can now look inside the structure of the full pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OZnSQzHjYx_"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "pz_gemma_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovWILtTvbkXr"
   },
   "source": [
    "And run it on the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtUZ6gBMhQWM"
   },
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_traceback_filtering\", 'off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUFXIA33bj57"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "pz_gemma_output = pz_gemma_model(GemmaInputs(\n",
    "    tokens=pz.nx.wrap(tokens[None, :]).tag(\"batch\", \"seq\"),\n",
    "    positions=pz.nx.wrap(positions).tag(\"batch\", \"seq\"),\n",
    "    attention_mask=pz.nx.wrap(attention_mask).tag(\"batch\", \"seq\", \"kv_seq\"),\n",
    "))\n",
    "pz_gemma_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pRr2g3plXa0"
   },
   "outputs": [],
   "source": [
    "chex.assert_trees_all_close(\n",
    "    flax_gemma_output,\n",
    "    pz_gemma_output.unwrap(\"batch\", \"seq\", \"vocabulary\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Q6B1qDW5e2i"
   },
   "source": [
    "And there we have it! A full port of Gemma to Penzai.\n",
    "\n",
    "Since all the internals are exposed, we can easily inspect arbitrary parts of this model or insert arbitrary logic. For instance, let's insert something in the middle of one of the MLPs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B_xzAZnoEhK"
   },
   "outputs": [],
   "source": [
    "# Copied by clicking in the above visualization:\n",
    "selector_fn = (lambda root: root.body.body.body.sublayers[13].sublayers[1].delta.sublayers[1].sublayers[1])\n",
    "# Insert our intermediate-printing \"Hello World\" layer from Section 1:\n",
    "patched_model = pz_gemma_model.select().at(selector_fn).insert_before(HelloWorld())\n",
    "# Look at it:\n",
    "patched_model.select().at_instances_of(HelloWorld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwX3HBt7ogj9"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "# Run the patched model:\n",
    "patched_model(GemmaInputs(\n",
    "    tokens=pz.nx.wrap(tokens[None, :]).tag(\"batch\", \"seq\"),\n",
    "    positions=pz.nx.wrap(positions).tag(\"batch\", \"seq\"),\n",
    "    attention_mask=pz.nx.wrap(attention_mask).tag(\"batch\", \"seq\", \"kv_seq\"),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw7A9DWAou_a"
   },
   "source": [
    "`patched_model` is an immutable copy of `pz_gemma_model` that includes our patching logic. Because of the functional nature of JAX (and Penzai), you never have to worry about rolling back patches or modifying hooks. And JAX automatically shares the array memory between the parameters of the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMqp4THa5snd"
   },
   "source": [
    "## Section 3: Adding support for KV-Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JYvEzhZ5xuT"
   },
   "source": [
    "So far, we've focused on the ordinary transformer forward pass, which would be used during training and for log-probability scoring. However, the Flax implementation of Gemma also allows you to do autoregressive sampling using key-value caching. In this section, we'll show how to add key-value caching to the model we've built so far, making it possible to efficiently run autoregressive sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBzPYsgOGUl0"
   },
   "outputs": [],
   "source": [
    "# Reload everything to ensure we have enough memory (for a TPU v2 kernel).\n",
    "for array in jax.live_arrays():\n",
    "  array.delete()\n",
    "\n",
    "pz_gemma_model = GemmaTransformer.from_pretrained(\n",
    "    gemma.params.nest_params(\n",
    "        gemma.params.param_remapper(\n",
    "            checkpointer.restore(ckpt_path, restore_args=restore_args)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "tokens = jnp.array([vocab.bos_id()] + vocab.EncodeAsIds(example_input))\n",
    "positions, attention_mask = get_attention_mask_and_positions(tokens[None, :], vocab.pad_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9c22_PdM0p0"
   },
   "source": [
    "### Penzai best practice: No conditional branching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBGnfD6jRw6N"
   },
   "source": [
    "Recall again the implementation of the attention block in the Flax version of Gemma, excerpted below:\n",
    "```python\n",
    "class Attention(nn.Module):\n",
    "  \"\"\"Attention module.\"\"\"\n",
    "  ...\n",
    "\n",
    "  def __call__(\n",
    "      self,\n",
    "      x: jax.Array,\n",
    "      segment_pos: jax.Array,\n",
    "      cache: LayerCache | None,\n",
    "      attn_mask: jax.Array,\n",
    "  ) -> tuple[LayerCache | None, jax.Array]:\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    if self.use_qkv_einsum:\n",
    "      query_proj, key_proj, value_proj = self.qkv_einsum('BTD,SNDH->SBTNH', x)\n",
    "    else:\n",
    "      query_proj = self.q_einsum('BTD,NDH->BTNH', x)\n",
    "      key_proj, value_proj = self.kv_einsum('BSD,CKDH->CBSKH', x)\n",
    "\n",
    "    query_proj = positional_embeddings.apply_rope(\n",
    "        query_proj, segment_pos, head_dim=self.head_dim,\n",
    "    )\n",
    "    query_scaled = query_proj * self.head_dim**-0.5\n",
    "    key_proj = positional_embeddings.apply_rope(\n",
    "        key_proj, segment_pos, head_dim=self.head_dim,\n",
    "    )\n",
    "\n",
    "    if not self.use_qkv_einsum:\n",
    "      value_proj = jnp.repeat(value_proj, self.num_heads, axis=-2)\n",
    "      key_proj = jnp.repeat(key_proj, self.num_heads, axis=-2)\n",
    "\n",
    "    if cache is not None:\n",
    "      end_index = cache['end_index'][0]\n",
    "      slice_indices = (0, end_index % cache['v'].shape[1], 0, 0)\n",
    "      value_proj = jax.lax.dynamic_update_slice(\n",
    "          cache['v'], value_proj, slice_indices,\n",
    "      )\n",
    "      key_proj = jax.lax.dynamic_update_slice(\n",
    "          cache['k'], key_proj, slice_indices\n",
    "      )\n",
    "\n",
    "    logits = jnp.einsum('BTNH,BSNH->BTNS', query_scaled, key_proj)\n",
    "    padded_logits = jnp.where((jnp.expand_dims(attn_mask, -2)), logits, K_MASK)\n",
    "    probs = jax.nn.softmax(padded_logits, axis=-1).astype(key_proj.dtype)\n",
    "    encoded = jnp.einsum('BTNS,BSNH->BTNH', probs, value_proj)\n",
    "    attn_output = self.attn_vec_einsum('BTNH,NHD->BTD', encoded)\n",
    "\n",
    "    if cache is not None:\n",
    "      new_cache = {'v': value_proj, 'k': key_proj, 'end_index': cache['end_index'] + seq_len}\n",
    "    else:\n",
    "      new_cache = None\n",
    "\n",
    "    return new_cache, attn_output\n",
    "```\n",
    "This implementation uses conditional branching in two places:\n",
    "- Depending on `use_qkv_einsum`, weights are either combined for queries, keys, and values, or kept separate.\n",
    "- Depending on whether the `cache` argument is passed, the `key_proj` and `value_proj` variables are either taken from the projection heads, or combined with the cache. This also determines whether a new cache is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhXFIGY_Scdn"
   },
   "source": [
    "This is fine for the Flax implementation, because the structure of the computation primarily lives in the code. However, this is not a common pattern in Penzai. In general, **idiomatic Penzai models should not contain Python conditional branches in their `__call__`** (except perhaps for shape checking or other assertions). This is a consequence of more general principles:\n",
    "- Idiomatic Penzai models should be \"what you see is what you get\"; it should be obvious what a model is going to do just by looking at its structure. Python conditional branching obscures this.\n",
    "- Idiomatic Penzai layers should be easy to patch and manipulate, which is easier if each layer does only one thing. Conditional branches usually mean your layer is doing multiple things depending on its configuration arguments.\n",
    "- JAX functions are usually JIT-compiled without Python control flow, and it's often useful for Penzai models to stay close to their JAX lowerings.\n",
    "\n",
    "How, then, do you get a single model to do multiple things, such as running in both training and kv-cache-based-sampling modes? Trick question! In general, you shouldn't have to do this in Penzai. Instead, the Penzai approach is to create a *copy* of the model that does a different thing.\n",
    "\n",
    "In fact, we've already seen this pattern at work! Every time we inserted logic to capture or print out intermediate values, we've been making a copy that does a different thing. Those patched models didn't use a conditional branch to decide whether or not to output intermediates. Instead, we just built new models that always outputs intermediates, by directly rewriting the model structure.\n",
    "\n",
    "Penzai's approach is centered on **hot-swapping**. Since layers make minimal assumptions about their children, we can implement different behaviors using *different layer classes* that have the same input and output structures. We can then have each of these implementations do a single thing, and still easily swap between the different implementations.\n",
    "\n",
    "In the case of our transformer, there are only two layers whose implementation needs to change in KV-cache mode:\n",
    "\n",
    "- The attention layer needs to be able to retrieve the keys and values from past tokens, compute attention over them, and update the key-value caches.\n",
    "- The top-level transformer wrapper needs to maintain the updated caches and make sure they stay in sync with the inputs.\n",
    "\n",
    "We'll assume the caller has correctly set up the positional embedding and attention mask side inputs so that they correctly reflect the current offsets. We'll also assume the input still has a \"seq\" axis. This can be of length 1 if we are sampling one at a time, but it can also be longer if we are prefilling the cache with a prompt.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sBPVsV9NNXM"
   },
   "source": [
    "### Adapting the attention block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBXcWvnUcz6l"
   },
   "source": [
    "The attention block is where most of the interesting work will happen. Our high-level goal is to build a new version of the `GemmaAttention` layer that is a drop-in replacement, but runs the KV caching logic instead of the ordinary logic. To handle the mutable KV cache, we'll use another one of Penzai's effects, `LocalState`, which allows us to get and set mutable state variables in a functional way.\n",
    "\n",
    "We'll re-use each of the same child blocks from the original `GemmaAttention` layer. None of them need to change; the only difference is that the inputs to `query_key_to_attn` and `attn_value_to_output` children will now have a `\"kv_seq\"` axis that is longer than the `\"seq\"` axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dfcwAcIddjb"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class GemmaKVCachingAttention(pz.Layer):\n",
    "  # Same as in GemmaAttention\n",
    "  input_to_query: pz.LayerLike\n",
    "  input_to_key: pz.LayerLike\n",
    "  input_to_value: pz.LayerLike\n",
    "  query_key_to_attn: pz.LayerLike\n",
    "  attn_value_to_output: pz.LayerLike\n",
    "\n",
    "  # New effects:\n",
    "  kv_cache_end_index: pz.de.SideInputEffect[jax.Array]\n",
    "  kv_cache: pz.de.LocalStateEffect[tuple[pz.nx.NamedArray, pz.nx.NamedArray]]\n",
    "\n",
    "  def __call__(self, x: pz.nx.NamedArray) -> pz.nx.NamedArray:\n",
    "    # Retrieve effectful inputs.\n",
    "    kvc_end_index = self.kv_cache_end_index.ask()\n",
    "    key_cache, value_cache = self.kv_cache.get()\n",
    "\n",
    "    # Compute queries, keys, and values as normal.\n",
    "    query = self.input_to_query(x)\n",
    "    key = self.input_to_key(x)\n",
    "    value = self.input_to_value(x)\n",
    "\n",
    "    # Update the KV caches.\n",
    "    new_key_cache = (\n",
    "        pz.nx.nmap(jax.lax.dynamic_update_slice)(\n",
    "            key_cache.untag(\"seq\"),\n",
    "            key.untag(\"seq\"),\n",
    "            (kvc_end_index,),\n",
    "        )\n",
    "        .tag(\"seq\")\n",
    "    )\n",
    "    new_value_cache = (\n",
    "        pz.nx.nmap(jax.lax.dynamic_update_slice)(\n",
    "            value_cache.untag(\"seq\"),\n",
    "            value.untag(\"seq\"),\n",
    "            (kvc_end_index,),\n",
    "        )\n",
    "        .tag(\"seq\")\n",
    "    )\n",
    "    self.kv_cache.set((new_key_cache, new_value_cache))\n",
    "\n",
    "    # Run the rest on the updated KV caches.\n",
    "    attn = self.query_key_to_attn((query, new_key_cache))\n",
    "    output = self.attn_value_to_output((attn, new_value_cache))\n",
    "    return output\n",
    "\n",
    "  @classmethod\n",
    "  def from_uncached(\n",
    "      cls,\n",
    "      original: GemmaAttention,\n",
    "      cache_len: int,\n",
    "      cached_axes: dict[str, int],  # <- We need this to initialize the cache.\n",
    "      cache_dtype: jax.typing.DTypeLike = jnp.float32,\n",
    "  ) -> GemmaKVCachingAttention:\n",
    "    \"\"\"Builds a cached attention from an uncached attention.\"\"\"\n",
    "\n",
    "    # Each layer that requests a state variable has to declare an initializer\n",
    "    # (or a concrete initial state) for it at the time that it's built.\n",
    "    def kv_cache_initializer():\n",
    "      empty_cache = pz.nx.zeros(\n",
    "          {**cached_axes, \"seq\": cache_len},\n",
    "          dtype=cache_dtype,\n",
    "      )\n",
    "      return (empty_cache, empty_cache)\n",
    "\n",
    "    return GemmaKVCachingAttention(\n",
    "        input_to_query=original.input_to_query,\n",
    "        input_to_key=original.input_to_key,\n",
    "        input_to_value=original.input_to_value,\n",
    "        query_key_to_attn=original.query_key_to_attn,\n",
    "        attn_value_to_output=original.attn_value_to_output,\n",
    "        kv_cache_end_index=pz.de.SideInputRequest(\"cache_end_index\"),\n",
    "        kv_cache=pz.de.InitialLocalStateRequest(\n",
    "            kv_cache_initializer, category=\"kv_cache\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg61P869zTWY"
   },
   "source": [
    "#### Hot-swapping attention blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18g_BQsvnMJS"
   },
   "source": [
    "We've set up our caching attention block so that it can instantiate itself as a copy of an initialized non-caching `GemmaAttention` block, so let's try grabbing one of the `GemmaAttention` blocks from the original transformer, and seeing if we can run it in cached decoding mode.\n",
    "\n",
    "We have to be a bit careful here, because we've already handled the `SideInput` effects in our Penzai Gemma model, and if we just remove a `GemmaAttention` block from the model we'll break the link between the handler and its references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9b_MUSDrUi7"
   },
   "outputs": [],
   "source": [
    "selector_fn = (lambda root: root.body.body.body.sublayers[2].sublayers[0].delta.sublayers[1])\n",
    "selector_fn(pz_gemma_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46WPJeANu5nB"
   },
   "source": [
    "Treescope shows us this with a \"`Broken handler refs`\" message.\n",
    "\n",
    "One option is to manually go in and replace these with unhandled effects by running something like\n",
    "```python\n",
    "(\n",
    "    pz.select(selector_fn(pz_gemma_model))\n",
    "    .at_instances_of(pz.de.HandledSideInputRef)\n",
    "    .where(lambda ref: ref.handler_id == 'WithSideInputsFromInputTuple_bc3f5')\n",
    "    .apply(lambda ref: pz.de.SideInputRequest(tag=ref.tag)\n",
    ")\n",
    "```\n",
    "This would let you handle them again normally.\n",
    "\n",
    "In this case, though, it's easier to use Penzai's built-in tools for capturing intermediate values, which lets us re-play the side inputs exactly as they appeared in the original model, and also lets us capture the input and output embeddings for this attention layer in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbaWuZgevwLA"
   },
   "outputs": [],
   "source": [
    "safely_extracted_attn = isolate_submodel.call_and_extract_submodel(\n",
    "    pz.select(pz_gemma_model).at(selector_fn),\n",
    "    GemmaInputs(\n",
    "        tokens=pz.nx.wrap(tokens[None, :]).tag(\"batch\", \"seq\"),\n",
    "        positions=pz.nx.wrap(positions).tag(\"batch\", \"seq\"),\n",
    "        attention_mask=pz.nx.wrap(attention_mask).tag(\"batch\", \"seq\", \"kv_seq\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjyGLqtKC01W"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "safely_extracted_attn.select().at_instances_of(GemmaAttention).at_children().show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3JvC1abwsZ7"
   },
   "source": [
    "As we can see above, the `GemmaAttention` block has extracted and placed into a new `WithSideInputsFromInputTuple` handler, and we can see all of the side inputs in the `saved_input` attribute.\n",
    "\n",
    "Let's swap out this captured attention block with our stateful one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUcmrkHdxaTK"
   },
   "outputs": [],
   "source": [
    "swapped_out_attn = (\n",
    "    pz.select(safely_extracted_attn.submodel)\n",
    "    .at_instances_of(GemmaAttention)\n",
    "    .apply(lambda attn: GemmaKVCachingAttention.from_uncached(\n",
    "        attn,\n",
    "        cache_len=58,\n",
    "        cached_axes={\n",
    "            \"batch\": 1,\n",
    "            \"projection\":pz_gemma_model.config.projection_dim,\n",
    "        },\n",
    "        cache_dtype=jnp.bfloat16,\n",
    "    ))\n",
    ")\n",
    "swapped_out_attn.select().at_instances_of(GemmaKVCachingAttention).at_children().show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyulO-epnkI2"
   },
   "source": [
    "To actually run this layer, we now need to handle two more effects (shown as the \"Unhandled effects\" annotation above): `SideInputEffect` and `LocalStateEffect`. We've also accidentally broken the positions and embedding\n",
    "\n",
    "We've seen how to handle `SideInputEffect`, so let's just get that out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxBI-d4qn2i_"
   },
   "outputs": [],
   "source": [
    "caching_attn_with_index = pz.de.WithSideInputsFromInputTuple.handling(\n",
    "    swapped_out_attn,\n",
    "    tags=[\"cache_end_index\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxCotSYToAbt"
   },
   "source": [
    "To handle the state effect, we need to convert our stateful model into a pure functional one. This means that, instead of mapping `x -> y`, it will map `(x, state_dict) -> (y, state_dict)`. Since state requests have their own initializers, we can separate our layer into a stateless function and an initial state dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEdZ07V_og_L"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "stateless_caching_attn_w_idx, state_dict = pz.de.handle_local_states(\n",
    "    caching_attn_with_index, category=\"kv_cache\"\n",
    ")\n",
    "print(\"Stateless model:\")\n",
    "pz.show(stateless_caching_attn_w_idx)\n",
    "print(\"State dict:\")\n",
    "pz.show(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gt6M5hYepMl7"
   },
   "source": [
    "We now need to call this layer with:\n",
    "  - embeddings, an attention mask, and token positions (saved from the original model)\n",
    "  - a cache offset (for our new side input handler)\n",
    "  - and a dictionary of states (in this case only one)\n",
    "\n",
    "Let's try running the model on the first half of the tokens (e.g. pre-filling the cache):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5jWWTr4pSxc"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer(maximum_size=10_000)\n",
    "cached_out, state_dict_1 = stateless_caching_attn_w_idx((\n",
    "    (\n",
    "        # The original input, which we'll slice along the `tokens` axis.\n",
    "        (\n",
    "            pz.select(safely_extracted_attn.saved_input)\n",
    "            .at_instances_of(pz.nx.NamedArray)\n",
    "            .apply(lambda arr: arr[{\"seq\": pz.slice[0:29]}])\n",
    "        ),\n",
    "        # The current cache offset (zero since we are at the start.)\n",
    "        0,\n",
    "    ),\n",
    "    # The initial state dictionary.\n",
    "    state_dict,\n",
    "))\n",
    "\n",
    "print(\"Cached out:\")\n",
    "pz.show(cached_out)\n",
    "print(\"New state dict:\")\n",
    "for k, v in state_dict_1.items():\n",
    "  print(f\"{k}:\")\n",
    "  pz.show(pz.ts.render_array(v[0], truncate=False))\n",
    "  pz.show(pz.ts.render_array(v[1], truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noq_XgNc_oW0"
   },
   "source": [
    "Now let's run it again and watch the states update. We'll slice the projection axis while we visualize it, so that we can focus on how the values change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dj93jTGp_r4v"
   },
   "outputs": [],
   "source": [
    "cur_state_dict = state_dict_1\n",
    "for timestep in range(29, 35):\n",
    "  step_out, cur_state_dict = stateless_caching_attn_w_idx((\n",
    "      (\n",
    "          # The original input, which we'll slice along the `tokens` axis.\n",
    "          (\n",
    "              pz.select(safely_extracted_attn.saved_input)\n",
    "              .at_instances_of(pz.nx.NamedArray)\n",
    "              .apply(lambda arr: arr[{\"seq\": pz.slice[timestep:timestep+1]}])\n",
    "          ),\n",
    "          # The new cache offset.\n",
    "          timestep,\n",
    "      ),\n",
    "      # The current state dictionary.\n",
    "      cur_state_dict,\n",
    "  ))\n",
    "\n",
    "  key_cache, value_cache = cur_state_dict[\"WithSideInputsFromInputTuple.body/WithSideInputsFromInputTuple.body/GemmaKVCachingAttention.kv_cache\"]\n",
    "\n",
    "  pz.show(\n",
    "      \"Step\", timestep, \"keys:\",\n",
    "      pz.ts.render_array(key_cache[{\"projection\": pz.slice[0:5]}], truncate=False),\n",
    "      \"\\nStep\", timestep, \"values:\",\n",
    "      pz.ts.render_array(value_cache[{\"projection\": pz.slice[0:5]}], truncate=False),\n",
    "      \"\\n--------------------------------\"\n",
    "  )\n",
    "\n",
    "pz.show(\n",
    "    \"Expected output\",\n",
    "    pz.ts.render_array(key_cache[{\"projection\": pz.slice[0:5]}], truncate=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8r2sYch9B72g"
   },
   "source": [
    "As desired, we're able to update the keys and values one token at a time. And we obtain the correct slice of the output embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgXpBhNOCqrJ"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "step_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u07lCtfxCjBD"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "safely_extracted_attn.saved_output[{\"seq\": pz.slice[34:35]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsLbONnMDQpF"
   },
   "source": [
    "Let's move on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnN9B92fDi6B"
   },
   "outputs": [],
   "source": [
    "del safely_extracted_attn, cur_state_dict, key_cache, value_cache, cached_out, state_dict_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80uvPOtONRbq"
   },
   "source": [
    "### Adapting the top-level Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vy6cF5NJJuh"
   },
   "source": [
    "We can now wrap this up in a convenient top-level interface by defining a new wrapper class, and providing a constructor method using the same hot-swapping strategy. Because we're using hot swapping and Penzai's effect system, we don't have to thread anything through the transformer blocks; we just swap out the attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6UmcEG5JSAm"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class GemmaKVCachingState(pz.Struct):\n",
    "  cache_len: int = field(metadata={\"pytree_node\": False})\n",
    "  batch_axes: dict[str, int] = field(metadata={\"pytree_node\": False})\n",
    "  kv_caches: dict[str, Any]\n",
    "  cache_end_index: int | jax.Array\n",
    "\n",
    "\n",
    "@pz.pytree_dataclass\n",
    "class GemmaKVCachingInputs(pz.Struct):\n",
    "  tokens: pz.nx.NamedArray\n",
    "  positions: pz.nx.NamedArray\n",
    "  attention_mask: pz.nx.NamedArray\n",
    "  sampling_state: GemmaKVCachingState\n",
    "\n",
    "\n",
    "@pz.pytree_dataclass\n",
    "class GemmaKVCachingTransformer(pz.Layer):\n",
    "  config: GemmaTransformerConfig = field(metadata={\"pytree_node\": False})\n",
    "  body: pz.LayerLike\n",
    "\n",
    "  def __call__(\n",
    "      self, inputs: GemmaKVCachingInputs\n",
    "  ) -> tuple[pz.nx.NamedArray, GemmaKVCachingState]:\n",
    "    outs, kv_caches = self.body((\n",
    "        (\n",
    "            (inputs.tokens, inputs.positions, inputs.attention_mask),\n",
    "            inputs.sampling_state.cache_end_index,\n",
    "        ),\n",
    "        inputs.sampling_state.kv_caches,\n",
    "    ))\n",
    "    return outs, GemmaKVCachingState(\n",
    "        cache_len=inputs.sampling_state.cache_len,\n",
    "        batch_axes=inputs.sampling_state.batch_axes,\n",
    "        kv_caches=kv_caches,\n",
    "        cache_end_index=(\n",
    "            inputs.sampling_state.cache_end_index\n",
    "            + inputs.tokens.named_shape[\"seq\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "  @classmethod\n",
    "  def from_uncached(\n",
    "      cls,\n",
    "      uncached: GemmaTransformer,\n",
    "      cache_len: int,\n",
    "      batch_axes: dict[str, int],  # <- We need this to initialize the cache.\n",
    "  ) -> tuple[GemmaKVCachingTransformer, GemmaKVCachingState]:\n",
    "    cached_axes = {\n",
    "        **batch_axes,\n",
    "        \"projection\": uncached.config.projection_dim,\n",
    "    }\n",
    "    if not uncached.config.single_kv_head:\n",
    "      cached_axes[\"heads\"] = uncached.config.num_heads\n",
    "    caching_body = (\n",
    "        pz.select(uncached.body)\n",
    "        .at_instances_of(GemmaAttention)\n",
    "        .apply(\n",
    "            lambda attn: GemmaKVCachingAttention.from_uncached(\n",
    "                attn,\n",
    "                cache_len=cache_len,\n",
    "                cached_axes=cached_axes,\n",
    "                cache_dtype=uncached.config.dtype,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    handled_body, initial_state = pz.de.handle_local_states(\n",
    "        pz.de.WithSideInputsFromInputTuple.handling(\n",
    "            caching_body, tags=[\"cache_end_index\"]\n",
    "        ),\n",
    "        category=\"kv_cache\",\n",
    "    )\n",
    "    inference_model = cls(config=uncached.config, body=handled_body)\n",
    "    sampling_state = GemmaKVCachingState(\n",
    "        cache_len=cache_len,\n",
    "        batch_axes=batch_axes,\n",
    "        kv_caches=initial_state,\n",
    "        cache_end_index=0,\n",
    "    )\n",
    "    return inference_model, sampling_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-sfDruhA2Xh"
   },
   "outputs": [],
   "source": [
    "inference_gemma, initial_inference_state = GemmaKVCachingTransformer.from_uncached(\n",
    "    pz_gemma_model,\n",
    "    cache_len=58,\n",
    "    batch_axes={\"batch\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjL_5u4PBP3j"
   },
   "outputs": [],
   "source": [
    "inference_gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNVub1y9BU2G"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "inference_gemma(GemmaKVCachingInputs(\n",
    "    tokens=pz.nx.wrap(tokens[None, :]).tag(\"batch\", \"seq\")[{\"seq\": pz.slice[0:28]}],\n",
    "    positions=pz.nx.wrap(positions).tag(\"batch\", \"seq\")[{\"seq\": pz.slice[0:28]}],\n",
    "    attention_mask=pz.nx.wrap(attention_mask).tag(\"batch\", \"seq\", \"kv_seq\")[{\"seq\": pz.slice[0:28]}],\n",
    "    sampling_state=initial_inference_state,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zbuh4SRNneb"
   },
   "source": [
    "### The sampling loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUEYWkgQCZ_j"
   },
   "source": [
    "Now that we have KV caching, we can use it as a building block for a sampling algorithm. Building a high-performance fully-featured sampler is out of scope for this tutorial, so let's just implement something simple.\n",
    "\n",
    "First, a prefilling function, which fills up a KV cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tYaYVTpC-rP"
   },
   "outputs": [],
   "source": [
    "def prefill(\n",
    "    model: GemmaKVCachingTransformer,\n",
    "    initial_sampling_state: GemmaKVCachingState,\n",
    "    prompt: pz.nx.NamedArray,\n",
    ") -> tuple[pz.nx.NamedArray, GemmaKVCachingState]:\n",
    "  # Token positions are just offsets along the token axis. For simplicity, we're\n",
    "  # assuming there's no padding to worry about, and that the prompt is a fixed\n",
    "  # length.\n",
    "  query_positions = pz.nx.arange(\"seq\", prompt.named_shape[\"seq\"])\n",
    "  # Tokens can attend to any kv-token position that they are after.\n",
    "  key_value_positions = pz.nx.arange(\"kv_seq\", initial_sampling_state.cache_len)\n",
    "  attention_mask = query_positions >= key_value_positions\n",
    "  # Run prefill:\n",
    "  out_logits, new_state = model(GemmaKVCachingInputs(\n",
    "      tokens=prompt,\n",
    "      positions=query_positions,\n",
    "      attention_mask=attention_mask,\n",
    "      sampling_state=initial_sampling_state,\n",
    "  ))\n",
    "  # Extract the log probs from the final token, since that determines the probs\n",
    "  # for the next sampled token.\n",
    "  final_token_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
    "      out_logits[{\"seq\": -1}].untag(\"vocabulary\")\n",
    "  ).tag(\"vocabulary\")\n",
    "  return final_token_log_probs, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKGnBhTmE9KH"
   },
   "outputs": [],
   "source": [
    "prompt_parts = [\n",
    "    vocab.EncodeAsIds(\"Penzai includes a number of general-purpose tools for analyzing JAX neural networks. It also includes a declarative neural-network library\"),\n",
    "    vocab.EncodeAsIds(\"JAX is Autograd and XLA, brought together for high-performance numerical computing. JAX provides a familiar NumPy-style API for ease of adoption by researchers and engineers.\"),\n",
    "    vocab.EncodeAsIds(\"Alice: Let's play 20 questions!\\nBob: Sure! Is it something I'd find in a house?\"),\n",
    "    vocab.EncodeAsIds(\"from __future__ import annotations\\nimport collections\\nimport contextlib\\nimport dataclasses\\nimport functools\\nimport itertools\\nimport typing\"),\n",
    "]\n",
    "prompt_parts = [ [vocab.bos_id()] + part[:24] for part in prompt_parts]\n",
    "for part in prompt_parts:\n",
    "  print(vocab.DecodeIds(part))\n",
    "  print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_HTe9xWNvlg"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "prompt = pz.nx.wrap(jnp.array(prompt_parts)).tag(\"batch\", \"seq\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiRr_8veN7O_"
   },
   "outputs": [],
   "source": [
    "inference_gemma, initial_inference_state = GemmaKVCachingTransformer.from_uncached(\n",
    "    pz_gemma_model,\n",
    "    cache_len=100,\n",
    "    batch_axes={\"batch\": 4},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xumgoDdFMPN"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "next_log_probs, sampling_state = prefill(inference_gemma, initial_inference_state, prompt)\n",
    "{\"next_log_probs\":next_log_probs, \"sampling_state\":sampling_state}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsmvjcbvK9WQ"
   },
   "source": [
    "Now let's write a function that advances one token at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGqK0zQXCqkA"
   },
   "outputs": [],
   "source": [
    "def advance_one_token(\n",
    "    model: GemmaKVCachingTransformer,\n",
    "    state: GemmaKVCachingState,\n",
    "    next_token: jax.Array,\n",
    ") -> tuple[pz.nx.NamedArray, GemmaKVCachingState]:\n",
    "  # Our query position is the current cache offset.\n",
    "  query_positions = pz.nx.wrap(state.cache_end_index)[{\"seq\": np.newaxis}]\n",
    "  # Tokens can attend to any kv-token position that they are after.\n",
    "  key_value_positions = pz.nx.arange(\"kv_seq\", state.cache_len)\n",
    "  attention_mask = query_positions >= key_value_positions\n",
    "  # Run and update just like before, but add a tokens axis:\n",
    "  out_logits, new_state = model(GemmaKVCachingInputs(\n",
    "      tokens=next_token[{\"seq\": np.newaxis}],\n",
    "      positions=query_positions,\n",
    "      attention_mask=attention_mask,\n",
    "      sampling_state=state,\n",
    "  ))\n",
    "  # Extract the log probs from this token.\n",
    "  final_token_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
    "      out_logits.untag(\"seq\").squeeze(0).untag(\"vocabulary\")\n",
    "  ).tag(\"vocabulary\")\n",
    "  return final_token_log_probs, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJKYOTaTMMBG"
   },
   "source": [
    "And now we can do a simple iterative loop to run our sampling.\n",
    "\n",
    "So far we've been running things in JAX's eager mode, but we can easily JIT compile the computation as well. Since every Penzai layer, input, and output is a PyTree, we can just wrap up our model in a `Jitted` combinator and it all just works. We can even still look inside the model, because `Jitted` is also just a PyTree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GwnqS_tQRDH"
   },
   "outputs": [],
   "source": [
    "from penzai.toolshed import jit_wrapper\n",
    "inference_gemma_jit = jit_wrapper.Jitted(inference_gemma)\n",
    "inference_gemma_jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPLxjYeUMNwK"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.key(0)\n",
    "next_log_probs, sampling_state = prefill(inference_gemma_jit, initial_inference_state, prompt)\n",
    "outputs = []\n",
    "\n",
    "while True:\n",
    "  rng, key = jax.random.split(rng)\n",
    "  # Split a key across named axes:\n",
    "  batched_keys = pz.nx.random_split(key, sampling_state.batch_axes)\n",
    "  next_token = pz.nx.nmap(jax.random.categorical)(\n",
    "      batched_keys, next_log_probs.untag(\"vocabulary\")\n",
    "  )\n",
    "  print([vocab.IdToPiece(int(tok)) for tok in next_token.unwrap(\"batch\").tolist()], end=\" \")\n",
    "  outputs.append(next_token)\n",
    "  # Are we done?\n",
    "  if sampling_state.cache_end_index >= sampling_state.cache_len:\n",
    "    break\n",
    "  next_log_probs, sampling_state = advance_one_token(inference_gemma_jit, sampling_state, next_token)\n",
    "\n",
    "stacked_outputs = pz.nx.stack(outputs, \"seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kptvUPCP-LT"
   },
   "outputs": [],
   "source": [
    "%%autovisualize pz.ts.ArrayAutovisualizer.for_tokenizer(vocab)\n",
    "stacked_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoQA9QCrSxkB"
   },
   "source": [
    "Let's see what Gemma 2B thinks the completions of our prompts could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnoSVOZAQ_ba"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "  prompt_str = vocab.DecodeIds(prompt.unwrap(\"batch\", \"seq\")[i, :].tolist())\n",
    "  completion_str = vocab.DecodeIds(stacked_outputs.unwrap(\"batch\", \"seq\")[i, :].tolist())\n",
    "  pz.show(pz.ts.bolded(prompt_str), wrap=True)\n",
    "  pz.show(pz.ts.with_color(completion_str, \"blue\"), wrap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f5b9ejcS2Sc"
   },
   "source": [
    "Seems like it's sampling something reasonable! This is a pretty small model and we're using temperature 1, so these aren't the highest-quality samples. But it at least means we've probably implemented sampling correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpVfrclWiXJz"
   },
   "source": [
    "### Intervening on sampling through patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPvQG1f7iZwY"
   },
   "source": [
    "Because of the declarative, functional design of our Gemma reimplementation, we can *still* look at and intervene on intermediate values even in our stateful JIT-compiled model! This means it's simple to try out modifications that would require a lot of work to set up in other sampling implementations.\n",
    "\n",
    "As an example, let's try knocking out a subset of the attention heads by forcing those heads to only attend to the beginning-of-sequence token. First, we define a simple layer that does the modification we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA0NkDzFiZYY"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class KnockOutAttentionHeads(pz.Layer):\n",
    "  head_mask: pz.nx.NamedArray\n",
    "  def __call__(self, attn_weights: pz.nx.NamedArray) -> pz.nx.NamedArray:\n",
    "    knocked_out_attn = pz.nx.wrap(\n",
    "        jnp.zeros(\n",
    "            [attn_weights.named_shape[\"kv_seq\"]],\n",
    "            attn_weights.dtype,\n",
    "        ).at[0].set(1.0)\n",
    "    ).tag(\"kv_seq\")\n",
    "    return pz.nx.nmap(jnp.where)(self.head_mask, attn_weights, knocked_out_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V7F8RbnjONX"
   },
   "outputs": [],
   "source": [
    "%%autovisualize\n",
    "KnockOutAttentionHeads(\n",
    "    head_mask=pz.nx.wrap(jnp.array([1,0,1,0,1,0,1,0])).tag(\"heads\")\n",
    ")(saved_attention_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3RUNHqBkBE5"
   },
   "source": [
    "Then we'll find the places we want to insert it. Let's knock out all the heads in some of the middle layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxePwd75kAtJ"
   },
   "outputs": [],
   "source": [
    "selection = (\n",
    "    pz.select(inference_gemma_jit)\n",
    "    .at_instances_of(GemmaKVCachingAttention)\n",
    "    .pick_nth_selected((7, 8, 9))\n",
    "    .at(lambda attn: attn.query_key_to_attn.sublayers[-1])\n",
    ")\n",
    "selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZGmcyBPkZAO"
   },
   "source": [
    "And create a patched copy of our inference model that includes our modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5c-3hJH_kZAP"
   },
   "outputs": [],
   "source": [
    "patched_inference = (\n",
    "    selection\n",
    "    .insert_after(KnockOutAttentionHeads(pz.nx.zeros({\"heads\": 8})))\n",
    ")\n",
    "pz.select(patched_inference).at_instances_of(KnockOutAttentionHeads).show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PmVrc7gkkUU"
   },
   "source": [
    "Now let's run our sampling loop again, but used our patched copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYIL6QT_kkUV"
   },
   "outputs": [],
   "source": [
    "rng = jax.random.key(0)\n",
    "next_log_probs, sampling_state = prefill(patched_inference, initial_inference_state, prompt)\n",
    "outputs = []\n",
    "\n",
    "while True:\n",
    "  rng, key = jax.random.split(rng)\n",
    "  # Split a key across named axes:\n",
    "  batched_keys = pz.nx.random_split(key, sampling_state.batch_axes)\n",
    "  next_token = pz.nx.nmap(jax.random.categorical)(\n",
    "      batched_keys, next_log_probs.untag(\"vocabulary\")\n",
    "  )\n",
    "  print([vocab.IdToPiece(int(tok)) for tok in next_token.unwrap(\"batch\").tolist()], end=\" \")\n",
    "  outputs.append(next_token)\n",
    "  # Are we done?\n",
    "  if sampling_state.cache_end_index >= sampling_state.cache_len:\n",
    "    break\n",
    "  next_log_probs, sampling_state = advance_one_token(patched_inference, sampling_state, next_token)\n",
    "\n",
    "stacked_outputs = pz.nx.stack(outputs, \"seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcP3nDbWkkUV"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "  prompt_str = vocab.DecodeIds(prompt.unwrap(\"batch\", \"seq\")[i, :].tolist())\n",
    "  completion_str = vocab.DecodeIds(stacked_outputs.unwrap(\"batch\", \"seq\")[i, :].tolist())\n",
    "  pz.show(pz.ts.bolded(prompt_str), wrap=True)\n",
    "  pz.show(pz.ts.with_color(completion_str, \"blue\"), wrap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shRfhMhHkloo"
   },
   "source": [
    "It's generally degraded, but still somewhat reasonable.\n",
    "\n",
    "What if we instead knock out the earliest attention heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTsaTYkHlybb"
   },
   "outputs": [],
   "source": [
    "def go_sample(patched_inference):\n",
    "  rng = jax.random.key(0)\n",
    "  next_log_probs, sampling_state = prefill(patched_inference, initial_inference_state, prompt)\n",
    "  outputs = []\n",
    "\n",
    "  while True:\n",
    "    rng, key = jax.random.split(rng)\n",
    "    batched_keys = pz.nx.random_split(key, sampling_state.batch_axes)\n",
    "    next_token = pz.nx.nmap(jax.random.categorical)(\n",
    "        batched_keys, next_log_probs.untag(\"vocabulary\")\n",
    "    )\n",
    "    outputs.append(next_token)\n",
    "    if sampling_state.cache_end_index >= sampling_state.cache_len:\n",
    "      break\n",
    "    next_log_probs, sampling_state = advance_one_token(patched_inference, sampling_state, next_token)\n",
    "\n",
    "  stacked_outputs = pz.nx.stack(outputs, \"seq\")\n",
    "\n",
    "  for i in range(4):\n",
    "    prompt_str = vocab.DecodeIds(prompt.unwrap(\"batch\", \"seq\")[i, :].tolist())\n",
    "    completion_str = vocab.DecodeIds(stacked_outputs.unwrap(\"batch\", \"seq\")[i, :].tolist())\n",
    "    pz.show(pz.ts.bolded(prompt_str), wrap=True)\n",
    "    pz.show(pz.ts.with_color(completion_str, \"blue\"), wrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oByDo6wUk0IM"
   },
   "outputs": [],
   "source": [
    "patched_inference = (\n",
    "    pz.select(inference_gemma_jit)\n",
    "    .at_instances_of(GemmaKVCachingAttention)\n",
    "    .pick_nth_selected((0, 1, 2))\n",
    "    .at(lambda attn: attn.query_key_to_attn.sublayers[-1])\n",
    "    .insert_after(KnockOutAttentionHeads(pz.nx.zeros({\"heads\": 8})))\n",
    ")\n",
    "go_sample(patched_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeWBpFctlbtq"
   },
   "source": [
    "Knocking out these heads seems to severely affect the generated samples, suggesting that the model is relying heavily on them to parse the prompt.\n",
    "\n",
    "What about the last heads?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKp5sE0Nlu6c"
   },
   "outputs": [],
   "source": [
    "patched_inference = (\n",
    "    pz.select(inference_gemma_jit)\n",
    "    .at_instances_of(GemmaKVCachingAttention)\n",
    "    .pick_nth_selected((15, 16, 17))\n",
    "    .at(lambda attn: attn.query_key_to_attn.sublayers[-1])\n",
    "    .insert_after(KnockOutAttentionHeads(pz.nx.zeros({\"heads\": 8})))\n",
    ")\n",
    "go_sample(patched_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5ocQsjlmPCw"
   },
   "source": [
    "Interestingly, knocking out the last attention heads seems to preserve the local coherence of the sample quite well, but we see a bit stronger drifts in the content.\n",
    "\n",
    "What about knocking out a quarter of the heads through the entire model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rfAEQQonmsR"
   },
   "outputs": [],
   "source": [
    "patched_inference = (\n",
    "    pz.select(inference_gemma_jit)\n",
    "    .at_instances_of(GemmaKVCachingAttention)\n",
    "    .at(lambda attn: attn.query_key_to_attn.sublayers[-1])\n",
    "    .insert_after(KnockOutAttentionHeads(pz.nx.wrap([1,1,1,1,1,1,0,0]).tag(\"heads\")))\n",
    ")\n",
    "go_sample(patched_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLONxkvvmb4b"
   },
   "source": [
    "These particular interventions aren't controlled enough to let us say anything definitive about what the model is doing. But this example demonstrates how Penzai's design principles make it possible to quickly patch model behavior, even at inference time.  Our simple KV-caching logic immediately supports arbitrary interventions to the sampling process, without us having to explicitly pre-define what changes we wanted to make or thread those changes through the model's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13aDIJhx5g6J"
   },
   "source": [
    "## Note: The structure of `penzai.example_models.gemma`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwUOsgOTyjoo"
   },
   "source": [
    "In addition to this notebook, Penzai also includes an implementation of Gemma in `penzai.example_models.gemma`, which is the recommended implementation to use when experimenting with Gemma using Penzai. This implementation is very similar to the implementation given here, with a few minor differences:\n",
    "\n",
    "- The classes `BranchAndMultiplyTogether`, `ApplyRoPE`, `ApplyAttentionMask`, `RMSStandardize`, `RMSLayerNorm`, `EmbeddingTable`, `EmbeddingLookup`, and`EmbeddingDecode`  have been moved into the Penzai standard library `pz.nn`, since they are not Gemma-specific.\n",
    "- The class `GemmaAttention` has been split into a base `Attention` combinator and a `GemmaAttention` subclass, to decouple the basic `Attention` control flow from the specific initialization logic of `GemmaAttention`.\n",
    "- Similarly the class `GemmaKVCachingAttention` has been split into a `KVCachingAttention` class in the standard library and a more specific `GemmaKVCachingAttention` layer. It includes an extra axis name attribute to make sure the base class does not make assumptions about axis names.\n",
    "- The top-level `GemmaTransformer` class's `from_pretrained` method has been modified to not depend on the Flax implementation's config, and instead loads directly from the `flat_params` checkpoint.\n",
    "- The top-level `GemmaTransformer` also supports computing activations in `float32` even when weights are `bfloat16`, to allow studying the activations in more detail.\n",
    "- The `GemmaInputs` and `GemmaKVCachingInputs` input structures have been extended with convenience methods to make them easier to set up for simple cases.\n",
    "- The `prefill` and `advance_one_token` have been extended to also support padding characters at the end of the prompt, which makes the attention mask and position computations slightly more complex. To track this, they use a `SamplingState` class that adds a few fields beyond `GemmaKVCachingState`.\n",
    "- Docstrings have been added with additional information on the design of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kqneh5PjqqFR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Gemma From Scratch: Inspecting and Re-implementing Gemma with Penzai",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
